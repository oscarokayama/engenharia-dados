{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56bc2e94",
   "metadata": {},
   "source": [
    "## 2. Documento de Testes (unitário e integrado)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196f7589",
   "metadata": {},
   "source": [
    "O processo de teste serve para validar o processo de carga de dados. Serão realizados teste unitário e teste integrado de cada uma das cargas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c59e168",
   "metadata": {},
   "source": [
    "**Teste unitário**\n",
    "\n",
    "O teste unitário valida se todas as linhas do arquivo foram inseridos no Data Lake.\n",
    "\n",
    "**Teste integrado**\n",
    "\n",
    "O teste integrado valida o a integridade de relacionamento entre os dados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6b029d",
   "metadata": {},
   "source": [
    "**Importação de bibliotecas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a49bf68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_unixtime, col, to_timestamp, coalesce\n",
    "from pyspark.sql.types import StringType, IntegerType, LongType, DecimalType, DateType"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58c1ee0",
   "metadata": {},
   "source": [
    "**Variaveis do projeto**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5eb4fa0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Diretorio dos arquivos csv\n",
    "v_diretorio_csv='/usr/local/spark/csv/'\n",
    "\n",
    "#Variaveis de conexao com postgres\n",
    "v_caminho_jar_postgres='/home/jovyan/work/jars/postgresql-9.4.1207.jar'\n",
    "v_url_jdbc='jdbc:postgresql://postgres/projeto'\n",
    "v_user_jdbc='airflow'\n",
    "v_pass_jdbc='airflow'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a62882",
   "metadata": {},
   "source": [
    "**Criando sessao e contexto**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce28e957",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.1.2-bin-hadoop3.2/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "22/12/22 17:46:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/12/22 17:46:37 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .master('local')\n",
    "         .appName('load-postgres')\n",
    "         # Add postgres jar\n",
    "         .config('spark.driver.extraClassPath', v_caminho_jar_postgres)\n",
    "         .getOrCreate())\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac95287",
   "metadata": {},
   "source": [
    "**Lendo arquivo csv, criando dataframe spark, formatando e criando views**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83fdfaee",
   "metadata": {},
   "source": [
    "Essa fase do processo, carrega os dados dos arquivos csv em dataframes, formata os campos e cria views para posteriormente serem utilizados na fase de tratamento dos dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8db463dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataframe Associado\n",
    "df_associado_csv = (\n",
    "    spark.read\n",
    "    .format('csv')\n",
    "    .option('header', True)\n",
    "    .option('delimiter', ';')\n",
    "    .load(v_diretorio_csv + 'associado.csv')\n",
    ")\n",
    "\n",
    "#Definindo o tipo da coluna\n",
    "df_associado_csv_fmt = (\n",
    "    df_associado_csv\n",
    "    .withColumn('id', col('id').cast(IntegerType()))\n",
    "    .withColumn('idade', col('idade').cast(IntegerType()))\n",
    ")\n",
    "\n",
    "#Criando view do dataframe\n",
    "df_associado_csv_fmt.createOrReplaceTempView('associado_csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27c15f34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+---------+-----+--------------------+\n",
      "| id|     nome|sobrenome|idade|               email|\n",
      "+---+---------+---------+-----+--------------------+\n",
      "|  1|   Alícia|  Cardoso|   29|alícia.cardoso@ho...|\n",
      "|  2|  Mirella|    Moura|   25|mirella.moura@gma...|\n",
      "|  3|  Rodrigo|Fernandes|   54|rodrigo.fernandes...|\n",
      "|  4|   Rebeca|  Cardoso|   59|rebeca.cardoso@te...|\n",
      "|  5|     Raul|   Barros|   51|raul.barros@yahoo...|\n",
      "|  6|    Julia|    Nunes|   38|julia.nunes@yahoo...|\n",
      "|  7|     João|   Miguel|   45|joão.miguel@uol.c...|\n",
      "|  8|Francisco|    Gomes|   27|francisco.gomes@h...|\n",
      "|  9| Vinicius|     Lima|   58|vinicius.lima@hot...|\n",
      "| 10|  Cecília|    Souza|   40|cecília.souza@uol...|\n",
      "| 11|      Ana|    Julia|   57|ana.julia@yahoo.c...|\n",
      "| 12|  Anthony|    Neves|   40|anthony.neves@yah...|\n",
      "| 13|    Lucas|    Costa|   34|lucas.costa@hotma...|\n",
      "| 14|      Ana| Teixeira|   66|ana.teixeira@hotm...|\n",
      "| 15|     João|    Lucas|   70|joão.lucas@uol.co...|\n",
      "| 16|    Bruna|      Luz|   69|bruna.luz@hotmail...|\n",
      "| 17|    Vitor|     Hugo|   67|vitor.hugo@hotmai...|\n",
      "| 18|    Sarah|Fernandes|   39|sarah.fernandes@y...|\n",
      "| 19|  Cecília|Rodrigues|   75|cecília.rodrigues...|\n",
      "| 20|   Nathan|     Mota|   42|nathan.mota@yahoo...|\n",
      "+---+---------+---------+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_associado_csv_fmt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62f45170",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataframe Conta\n",
    "df_conta_csv = (\n",
    "    spark.read\n",
    "    .format('csv')\n",
    "    .option('header', True)\n",
    "    .option('delimiter', ';')\n",
    "    .load(v_diretorio_csv + 'conta.csv')\n",
    ")\n",
    "\n",
    "#Definindo o tipo da coluna\n",
    "df_conta_csv_fmt = (\n",
    "    df_conta_csv\n",
    "    .withColumn('id', col('id').cast(IntegerType()))\n",
    "    .withColumn('data_criacao', col('data_criacao').cast(DateType()))\n",
    "    .withColumn('id_associado', col('id_associado').cast(IntegerType()))\n",
    ")\n",
    "\n",
    "#Criando view do dataframe\n",
    "df_conta_csv_fmt.createOrReplaceTempView('conta_csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67ec4181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+------------+------------+\n",
      "| id|          tipo|data_criacao|id_associado|\n",
      "+---+--------------+------------+------------+\n",
      "|  1|Conta Corrente|  2019-03-28|           1|\n",
      "|  2|Conta Corrente|  2021-04-02|           2|\n",
      "|  3|Conta Corrente|  2019-05-24|           3|\n",
      "|  4|Conta Corrente|  2018-10-22|           4|\n",
      "|  5|Conta Corrente|  2022-11-29|           5|\n",
      "|  6|Conta Corrente|  2018-05-26|           6|\n",
      "|  7|Conta Corrente|  2020-08-23|           7|\n",
      "|  8|Conta Corrente|  2019-02-16|           8|\n",
      "|  9|Conta Corrente|  2021-03-09|           9|\n",
      "| 10|Conta Corrente|  2022-04-09|          10|\n",
      "| 11|Conta Corrente|  2019-10-08|          11|\n",
      "| 12|Conta Corrente|  2022-04-28|          12|\n",
      "| 13|Conta Corrente|  2019-02-15|          13|\n",
      "| 14|Conta Corrente|  2022-08-21|          14|\n",
      "| 15|Conta Corrente|  2022-07-15|          15|\n",
      "| 16|Conta Corrente|  2019-12-27|          16|\n",
      "| 17|Conta Corrente|  2022-07-31|          17|\n",
      "| 18|Conta Corrente|  2018-07-13|          18|\n",
      "| 19|Conta Corrente|  2019-04-14|          19|\n",
      "| 20|Conta Corrente|  2022-12-07|          20|\n",
      "+---+--------------+------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_conta_csv_fmt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e780167",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataframe Cartao\n",
    "df_cartao_csv = (\n",
    "    spark.read\n",
    "    .format('csv')\n",
    "    .option('header', True)\n",
    "    .option('delimiter', ';')\n",
    "    .load(v_diretorio_csv + 'cartao.csv')\n",
    ")\n",
    "\n",
    "#Definindo o tipo da coluna\n",
    "df_cartao_csv_fmt = (\n",
    "    df_cartao_csv\n",
    "    .withColumn('id', col('id').cast(IntegerType()))\n",
    "    .withColumn('id_conta', col('id_conta').cast(IntegerType()))\n",
    "    .withColumn('id_associado', col('id_associado').cast(IntegerType()))\n",
    ")\n",
    "\n",
    "#Criando view do dataframe\n",
    "df_cartao_csv_fmt.createOrReplaceTempView('cartao_csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22095102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+-----------------+--------+------------+\n",
      "| id|      num_cartao|     nom_impresso|id_conta|id_associado|\n",
      "+---+----------------+-----------------+--------+------------+\n",
      "|  1|8692002900010397|   ALÍCIA CARDOSO|       1|           1|\n",
      "|  2|1360002500020347|    MIRELLA MOURA|       2|           2|\n",
      "|  3|3935005400035103|RODRIGO FERNANDES|       3|           3|\n",
      "|  4|4371005900041388|   REBECA CARDOSO|       4|           4|\n",
      "|  5|9500005100053578|      RAUL BARROS|       5|           5|\n",
      "|  6|7915003800066514|      JULIA NUNES|       6|           6|\n",
      "|  7|2184004500079616|      JOÃO MIGUEL|       7|           7|\n",
      "|  8|2631002700088038|  FRANCISCO GOMES|       8|           8|\n",
      "|  9|3191005800091087|    VINICIUS LIMA|       9|           9|\n",
      "| 10|9897004000108416|    CECÍLIA SOUZA|      10|          10|\n",
      "| 11|8684005700115334|        ANA JULIA|      11|          11|\n",
      "| 12|8694004000128933|    ANTHONY NEVES|      12|          12|\n",
      "| 13|9950003400138288|      LUCAS COSTA|      13|          13|\n",
      "| 14|4373006600142001|     ANA TEIXEIRA|      14|          14|\n",
      "| 15|6333007000157004|       JOÃO LUCAS|      15|          15|\n",
      "| 16|9080006900166160|        BRUNA LUZ|      16|          16|\n",
      "| 17|6279006700177996|       VITOR HUGO|      17|          17|\n",
      "| 18|5432003900184311|  SARAH FERNANDES|      18|          18|\n",
      "| 19|4667007500196222|CECÍLIA RODRIGUES|      19|          19|\n",
      "| 20|5578004200205193|      NATHAN MOTA|      20|          20|\n",
      "+---+----------------+-----------------+--------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_cartao_csv_fmt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e2efcb9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataframe Movimento\n",
    "df_movimento_csv = (\n",
    "    spark.read\n",
    "    .format(\"csv\")\n",
    "    .option(\"header\", True)\n",
    "    .option(\"delimiter\", \";\")\n",
    "    .load(v_diretorio_csv + \"movimento.csv\")\n",
    ")\n",
    "\n",
    "#Definindo o tipo da coluna\n",
    "df_movimento_csv_fmt = (\n",
    "    df_movimento_csv\n",
    "    .withColumn('id', col('id').cast(IntegerType()))\n",
    "    .withColumn('vlr_transacao', col('vlr_transacao').cast(DecimalType(10,2)))\n",
    "    .withColumn('data_movimento', col('data_movimento').cast(DateType()))\n",
    "    .withColumn('id_cartao', col('id_cartao').cast(IntegerType()))\n",
    ")\n",
    "\n",
    "#Criando view do dataframe\n",
    "df_movimento_csv_fmt.createOrReplaceTempView('movimento_csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "94209042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+-----------------+--------------+---------+\n",
      "|    id|vlr_transacao|    des_transacao|data_movimento|id_cartao|\n",
      "+------+-------------+-----------------+--------------+---------+\n",
      "|288984|       195.88|      Restaurante|    2020-03-13|     1647|\n",
      "|288985|       241.41|         Farmacia|    2020-03-16|     1647|\n",
      "|288986|       919.07|Posto combustivel|    2020-03-19|     1647|\n",
      "|288987|       470.36|Posto combustivel|    2020-03-23|     1647|\n",
      "|288988|       165.36|            Roupa|    2020-03-25|     1647|\n",
      "|288989|       395.24|Posto combustivel|    2020-03-28|     1647|\n",
      "|288990|       415.97|Posto combustivel|    2020-04-07|     1647|\n",
      "|288991|       587.15|         Pet shop|    2020-04-08|     1647|\n",
      "|288992|       468.72|     Supermercado|    2020-04-14|     1647|\n",
      "|288993|       476.72|         Pet shop|    2020-04-15|     1647|\n",
      "|288994|        87.02|     Supermercado|    2020-04-16|     1647|\n",
      "|288995|       560.57|            Roupa|    2020-04-25|     1647|\n",
      "|288996|       389.13|         Pet shop|    2020-04-26|     1647|\n",
      "|288997|       444.85|Posto combustivel|    2020-04-29|     1647|\n",
      "|288998|        32.18|      Restaurante|    2020-05-03|     1647|\n",
      "|288999|       449.23|Posto combustivel|    2020-05-08|     1647|\n",
      "|289000|       445.97|            Roupa|    2020-05-11|     1647|\n",
      "|289001|        86.89|Posto combustivel|    2020-05-14|     1647|\n",
      "|289002|       357.41|      Restaurante|    2020-05-19|     1647|\n",
      "|289003|       213.26|Posto combustivel|    2020-05-23|     1647|\n",
      "+------+-------------+-----------------+--------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_movimento_csv_fmt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f3a0e74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataframe Encerramento\n",
    "df_encerramento_csv = (\n",
    "    spark.read\n",
    "    .format('csv')\n",
    "    .option('header', True)\n",
    "    .option('delimiter', ';')\n",
    "    .load(v_diretorio_csv + 'encerramento_conta.csv')\n",
    ")\n",
    "\n",
    "#Removendo colunas\n",
    "new_df_encerramento_csv=df_encerramento_csv.drop('semente', 'data_parou_comprar', 'dias_sem_compra')\n",
    "\n",
    "#Definindo o tipo da coluna\n",
    "df_encerramento_csv_fmt = (\n",
    "    new_df_encerramento_csv\n",
    "    .withColumn('id', col('id').cast(IntegerType()))\n",
    "    .withColumn('data_criacao', col('data_criacao').cast(DateType()))\n",
    "    .withColumn('data_encerramento', col('data_encerramento').cast(DateType()))\n",
    ")\n",
    "\n",
    "#Criando view do dataframe\n",
    "df_encerramento_csv_fmt.createOrReplaceTempView('encerramento_conta_csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d9dc0594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+-----------------+\n",
      "| id|data_criacao|data_encerramento|\n",
      "+---+------------+-----------------+\n",
      "|  1|  2019-03-28|             null|\n",
      "|  2|  2021-04-02|             null|\n",
      "|  3|  2019-05-24|             null|\n",
      "|  4|  2018-10-22|             null|\n",
      "|  5|  2022-11-29|             null|\n",
      "|  6|  2018-05-26|             null|\n",
      "|  7|  2020-08-23|             null|\n",
      "|  8|  2019-02-16|             null|\n",
      "|  9|  2021-03-09|             null|\n",
      "| 10|  2022-04-09|             null|\n",
      "| 11|  2019-10-08|             null|\n",
      "| 12|  2022-04-28|             null|\n",
      "| 13|  2019-02-15|             null|\n",
      "| 14|  2022-08-21|             null|\n",
      "| 15|  2022-07-15|             null|\n",
      "| 16|  2019-12-27|             null|\n",
      "| 17|  2022-07-31|             null|\n",
      "| 18|  2018-07-13|             null|\n",
      "| 19|  2019-04-14|             null|\n",
      "| 20|  2022-12-07|             null|\n",
      "+---+------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_encerramento_csv_fmt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e0abba32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataframe Fatura\n",
    "df_fatura_csv = (\n",
    "    spark.read\n",
    "    .format('csv')\n",
    "    .option('header', True)\n",
    "    .option('delimiter', ';')\n",
    "    .load(v_diretorio_csv + 'fatura.csv')\n",
    ")\n",
    "\n",
    "#Definindo o tipo da coluna\n",
    "df_fatura_csv_fmt = (\n",
    "    df_fatura_csv\n",
    "    .withColumn('id', col('id').cast(IntegerType()))\n",
    "    .withColumn('data_vencimento_fatura', col('data_vencimento_fatura').cast(DateType()))\n",
    "    .withColumn('vlr_fatura', col('vlr_fatura').cast(DecimalType(10,2)))\n",
    "    .withColumn('data_pagamento_fatura', col('data_pagamento_fatura').cast(DateType()))\n",
    "    .withColumn('qtd_dias_atraso_pgto', col('qtd_dias_atraso_pgto').cast(IntegerType()))\n",
    "    .withColumn('id_cartao', col('id_cartao').cast(IntegerType()))\n",
    ")\n",
    "\n",
    "#Criando view do dataframe\n",
    "df_fatura_csv_fmt.createOrReplaceTempView('fatura_csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "65f118eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------------+----------+---------------------+--------------------+---------+\n",
      "| id|data_vencimento_fatura|vlr_fatura|data_pagamento_fatura|qtd_dias_atraso_pgto|id_cartao|\n",
      "+---+----------------------+----------+---------------------+--------------------+---------+\n",
      "|  1|            2019-03-15|      0.00|           2019-03-15|                   0|        1|\n",
      "|  2|            2019-04-15|    290.45|           2019-04-15|                   0|        1|\n",
      "|  3|            2019-05-15|    424.55|           2019-05-14|                   0|        1|\n",
      "|  4|            2019-06-15|    974.09|           2019-06-12|                   0|        1|\n",
      "|  5|            2019-07-15|    156.41|           2019-07-15|                   0|        1|\n",
      "|  6|            2019-08-15|    257.26|           2019-08-15|                   0|        1|\n",
      "|  7|            2019-09-15|    917.77|           2019-09-12|                   0|        1|\n",
      "|  8|            2019-10-15|    133.05|           2019-10-15|                   0|        1|\n",
      "|  9|            2019-11-15|    489.37|           2019-11-14|                   0|        1|\n",
      "| 10|            2019-12-15|    248.31|           2019-12-15|                   0|        1|\n",
      "| 11|            2020-01-15|   1474.16|           2020-01-12|                   0|        1|\n",
      "| 12|            2020-02-15|   1787.91|           2020-02-11|                   0|        1|\n",
      "| 13|            2020-03-15|   1785.06|           2020-03-13|                   0|        1|\n",
      "| 14|            2020-04-15|   1051.39|           2020-04-14|                   0|        1|\n",
      "| 15|            2020-05-15|    617.40|           2020-05-12|                   0|        1|\n",
      "| 16|            2020-06-15|    486.46|           2020-06-10|                   0|        1|\n",
      "| 17|            2020-07-15|    835.28|           2020-07-12|                   0|        1|\n",
      "| 18|            2020-08-15|    389.56|           2020-08-14|                   0|        1|\n",
      "| 19|            2020-09-15|   1164.28|           2020-09-11|                   0|        1|\n",
      "| 20|            2020-10-15|    682.20|           2020-10-10|                   0|        1|\n",
      "+---+----------------------+----------+---------------------+--------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_fatura_csv_fmt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b5f3b7",
   "metadata": {},
   "source": [
    "**Carregando dataframes e views com os dados do banco de dados do target**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7c9289d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Carregando dados no Dataframe\n",
    "df_associado_tgt = (\n",
    "    spark.read\n",
    "    .format('jdbc')\n",
    "    .option('url', v_url_jdbc)\n",
    "    .option('dbtable', 'target.associado')\n",
    "    .option('user', v_user_jdbc)\n",
    "    .option('password', v_pass_jdbc)\n",
    "    .load()\n",
    ")\n",
    "\n",
    "#Criando view do dataframe\n",
    "df_associado_tgt.createOrReplaceTempView('associado_tgt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9189b9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Carregando dados no Dataframe\n",
    "df_conta_tgt = (\n",
    "    spark.read\n",
    "    .format('jdbc')\n",
    "    .option('url', v_url_jdbc)\n",
    "    .option('dbtable', 'target.conta')\n",
    "    .option('user', v_user_jdbc)\n",
    "    .option('password', v_pass_jdbc)\n",
    "    .load()\n",
    ")\n",
    "\n",
    "#Criando view do dataframe\n",
    "df_conta_tgt.createOrReplaceTempView('conta_tgt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a8e366ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Carregando dados no Dataframe\n",
    "df_cartao_tgt = (\n",
    "    spark.read\n",
    "    .format('jdbc')\n",
    "    .option('url', v_url_jdbc)\n",
    "    .option('dbtable', 'target.cartao')\n",
    "    .option('user', v_user_jdbc)\n",
    "    .option('password', v_pass_jdbc)\n",
    "    .load()\n",
    ")\n",
    "\n",
    "#Criando view do dataframe\n",
    "df_cartao_tgt.createOrReplaceTempView('cartao_tgt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d2c93111",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Carregando dados no Dataframe\n",
    "df_movimento_tgt = (\n",
    "    spark.read\n",
    "    .format('jdbc')\n",
    "    .option('url', v_url_jdbc)\n",
    "    .option('dbtable', 'target.movimento')\n",
    "    .option('user', v_user_jdbc)\n",
    "    .option('password', v_pass_jdbc)\n",
    "    .load()\n",
    ")\n",
    "\n",
    "#Criando view do dataframe\n",
    "df_movimento_tgt.createOrReplaceTempView('movimento_tgt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c0cfe480",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Carregando dados no Dataframe\n",
    "df_encerramento_conta_tgt = (\n",
    "    spark.read\n",
    "    .format('jdbc')\n",
    "    .option('url', v_url_jdbc)\n",
    "    .option('dbtable', 'target.encerramento_conta')\n",
    "    .option('user', v_user_jdbc)\n",
    "    .option('password', v_pass_jdbc)\n",
    "    .load()\n",
    ")\n",
    "\n",
    "#Criando view do dataframe\n",
    "df_encerramento_conta_tgt.createOrReplaceTempView('encerramento_conta_tgt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ed99aa56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Carregando dados no Dataframe\n",
    "df_fatura_tgt = (\n",
    "    spark.read\n",
    "    .format('jdbc')\n",
    "    .option('url', v_url_jdbc)\n",
    "    .option('dbtable', 'target.fatura')\n",
    "    .option('user', v_user_jdbc)\n",
    "    .option('password', v_pass_jdbc)\n",
    "    .load()\n",
    ")\n",
    "\n",
    "#Criando view do dataframe\n",
    "df_fatura_tgt.createOrReplaceTempView('fatura_tgt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd10d34",
   "metadata": {},
   "source": [
    "### Teste unitário"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31aa392",
   "metadata": {},
   "source": [
    "O objetivo do teste unitário, é validar se todos os dados do arquivo csv foram carregados corretamente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9439e47d",
   "metadata": {},
   "source": [
    "**Arquivo associado**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "37de595e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validação da quantidade de registros: CSV x Target (Data Lake)\n",
      "\n",
      "Qtd. registros do arquivo csv: 9951.\n",
      "Qtd. registros encontrados no target: 9951. \n",
      "\n",
      "Comparação de 20 registros aleatórios\n",
      "+------+------+---------+---------+-------------+-------------+---------+---------+--------------------+--------------------+\n",
      "|id_csv|id_tgt| nome_csv| nome_tgt|sobrenome_csv|sobrenome_tgt|idade_csv|idade_tgt|           email_csv|           email_tgt|\n",
      "+------+------+---------+---------+-------------+-------------+---------+---------+--------------------+--------------------+\n",
      "|     3|     3|  Rodrigo|  Rodrigo|    Fernandes|    Fernandes|       54|       54|rodrigo.fernandes...|rodrigo.fernandes...|\n",
      "|    10|    10|  Cecília|  Cecília|        Souza|        Souza|       40|       40|cecília.souza@uol...|cecília.souza@uol...|\n",
      "|    17|    17|    Vitor|    Vitor|         Hugo|         Hugo|       67|       67|vitor.hugo@hotmai...|vitor.hugo@hotmai...|\n",
      "|    27|    27|      Ana|      Ana|        Luiza|        Luiza|       27|       27| ana.luiza@gmail.com| ana.luiza@gmail.com|\n",
      "|    47|    47|    Diogo|    Diogo|        Cunha|        Cunha|       70|       70|diogo.cunha@terra...|diogo.cunha@terra...|\n",
      "|    58|    58|   Thiago|   Thiago|    Rodrigues|    Rodrigues|       37|       37|thiago.rodrigues@...|thiago.rodrigues@...|\n",
      "|    59|    59|     João|     João|        Lucas|        Lucas|       40|       40|joão.lucas@gmail.com|joão.lucas@gmail.com|\n",
      "|    67|    67|Gabrielly|Gabrielly|      Ribeiro|      Ribeiro|       48|       48|gabrielly.ribeiro...|gabrielly.ribeiro...|\n",
      "|    80|    80|   Esther|   Esther|          Luz|          Luz|       66|       66|esther.luz@yahoo....|esther.luz@yahoo....|\n",
      "|    93|    93|  Cecília|  Cecília|     Silveira|     Silveira|       23|       23|cecília.silveira@...|cecília.silveira@...|\n",
      "|   115|   115|Gabrielly|Gabrielly|        Neves|        Neves|       51|       51|gabrielly.neves@t...|gabrielly.neves@t...|\n",
      "|   127|   127|    Diego|    Diego|        Souza|        Souza|       45|       45|diego.souza@gmail...|diego.souza@gmail...|\n",
      "|   139|   139|      Ana|      Ana|        Clara|        Clara|       25|       25|ana.clara@terra.c...|ana.clara@terra.c...|\n",
      "|   141|   141|    Srta.|    Srta.|          Ana|          Ana|       31|       31|srta..ana@hotmail...|srta..ana@hotmail...|\n",
      "|   146|   146|  Anthony|  Anthony|        Sales|        Sales|       63|       63|anthony.sales@hot...|anthony.sales@hot...|\n",
      "|   149|   149|   Sophia|   Sophia|       Campos|       Campos|       74|       74|sophia.campos@yah...|sophia.campos@yah...|\n",
      "|   171|   171|     João|     João|    Guilherme|    Guilherme|       20|       20|joão.guilherme@ya...|joão.guilherme@ya...|\n",
      "|   178|   178|   Daniel|   Daniel|     Silveira|     Silveira|       29|       29|daniel.silveira@g...|daniel.silveira@g...|\n",
      "|   182|   182|     Raul|     Raul|      Moreira|      Moreira|       23|       23|raul.moreira@gmai...|raul.moreira@gmai...|\n",
      "|   185|   185|   Emilly|   Emilly|       Barros|       Barros|       72|       72|emilly.barros@hot...|emilly.barros@hot...|\n",
      "+------+------+---------+---------+-------------+-------------+---------+---------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_associado_validacao=spark.sql('''\n",
    "    select \n",
    "        csv.id as id_csv,\n",
    "        tgt.id as id_tgt,\n",
    "        csv.nome as nome_csv,\n",
    "        tgt.nome as nome_tgt,\n",
    "        csv.sobrenome as sobrenome_csv,\n",
    "        tgt.sobrenome as sobrenome_tgt,\n",
    "        csv.idade as idade_csv,\n",
    "        tgt.idade as idade_tgt,\n",
    "        csv.email as email_csv,\n",
    "        tgt.email as email_tgt\n",
    "        \n",
    "    from associado_csv csv\n",
    "    \n",
    "    inner join associado_tgt tgt\n",
    "    on tgt.id=csv.id\n",
    "''')\n",
    "\n",
    "qtd_associados_csv=df_associado_csv.count()\n",
    "qtd_associados_encontrados=df_associado_validacao.count()\n",
    "\n",
    "print(\"Validação da quantidade de registros: CSV x Target (Data Lake)\")\n",
    "print()\n",
    "print(f\"Qtd. registros do arquivo csv: {qtd_associados_csv}.\")\n",
    "print(f\"Qtd. registros encontrados no target: {qtd_associados_encontrados}. \")\n",
    "print()\n",
    "print(\"Comparação de 20 registros aleatórios\")\n",
    "df_associado_validacao.sample(False, 0.1, seed=0).limit(20).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fd1c8e",
   "metadata": {},
   "source": [
    "**Arquivo conta**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "80b94828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validação da quantidade de registros: CSV x Target (Data Lake)\n",
      "\n",
      "Qtd. registros do arquivo csv: 10001.\n",
      "Qtd. registros encontrados no target: 10001. \n",
      "\n",
      "Comparação de 20 registros aleatórios\n",
      "+------+------+--------------+--------------+----------------+----------------+----------------+----------------+\n",
      "|id_csv|id_tgt|      tipo_csv|      tipo_tgt|data_criacao_csv|data_criacao_tgt|id_associado_csv|id_associado_tgt|\n",
      "+------+------+--------------+--------------+----------------+----------------+----------------+----------------+\n",
      "|   471|   471|Conta Corrente|Conta Corrente|      2018-04-12|      2018-04-12|             471|             471|\n",
      "|  1591|  1591|Conta Corrente|Conta Corrente|      2021-02-01|      2021-02-01|            1591|            1591|\n",
      "|  2659|  2659|Conta Corrente|Conta Corrente|      2018-05-26|      2018-05-26|            2659|            2659|\n",
      "|  4900|  4900|Conta Corrente|Conta Corrente|      2021-10-24|      2021-10-24|            4900|            4900|\n",
      "|  7982|  7982|Conta Corrente|Conta Corrente|      2021-07-14|      2021-07-14|            7982|            7982|\n",
      "|   243|   243|Conta Corrente|Conta Corrente|      2022-02-09|      2022-02-09|             243|             243|\n",
      "|   392|   392|Conta Corrente|Conta Corrente|      2020-07-30|      2020-07-30|             392|              -1|\n",
      "|  1127|  1127|Conta Corrente|Conta Corrente|      2020-03-16|      2020-03-16|            1127|            1127|\n",
      "|  2811|  2811|Conta Corrente|Conta Corrente|      2020-04-01|      2020-04-01|            2811|            2811|\n",
      "|  6623|  6623|Conta Corrente|Conta Corrente|      2020-08-05|      2020-08-05|            6623|            6623|\n",
      "|  1699|  1699|Conta Corrente|Conta Corrente|      2018-01-21|      2018-01-21|            1699|            1699|\n",
      "|  3704|  3704|Conta Corrente|Conta Corrente|      2021-06-09|      2021-06-09|            3704|            3704|\n",
      "|  6559|  6559|Conta Corrente|Conta Corrente|      2022-07-27|      2022-07-27|            6559|            6559|\n",
      "|  6825|  6825|Conta Corrente|Conta Corrente|      2021-04-02|      2021-04-02|            6825|            6825|\n",
      "|  8222|  8222|Conta Corrente|Conta Corrente|      2022-10-05|      2022-10-05|            8222|            8222|\n",
      "|  8924|  8924|Conta Corrente|Conta Corrente|      2022-03-12|      2022-03-12|            8924|            8924|\n",
      "|  2748|  2748|Conta Corrente|Conta Corrente|      2019-05-03|      2019-05-03|            2748|            2748|\n",
      "|  3876|  3876|Conta Corrente|Conta Corrente|      2022-08-09|      2022-08-09|            3876|            3876|\n",
      "|  4186|  4186|Conta Corrente|Conta Corrente|      2022-04-20|      2022-04-20|            4186|            4186|\n",
      "|  4684|  4684|Conta Corrente|Conta Corrente|      2019-10-07|      2019-10-07|            4684|            4684|\n",
      "+------+------+--------------+--------------+----------------+----------------+----------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_conta_validacao=spark.sql('''\n",
    "    select \n",
    "         csv.id as id_csv,\n",
    "         tgt.id as id_tgt,\n",
    "         csv.tipo as tipo_csv,\n",
    "         tgt.tipo as tipo_tgt,\n",
    "         csv.data_criacao as data_criacao_csv,\n",
    "         tgt.data_criacao as data_criacao_tgt,\n",
    "         csv.id_associado as id_associado_csv,\n",
    "         tgt.id_associado as id_associado_tgt\n",
    "        \n",
    "    from conta_csv csv\n",
    "    \n",
    "    inner join conta_tgt tgt\n",
    "    on tgt.id=csv.id\n",
    "''')\n",
    "\n",
    "qtd_conta_csv=df_conta_csv.count()\n",
    "qtd_conta_encontrados=df_conta_validacao.count()\n",
    "\n",
    "print(\"Validação da quantidade de registros: CSV x Target (Data Lake)\")\n",
    "print()\n",
    "print(f\"Qtd. registros do arquivo csv: {qtd_conta_csv}.\")\n",
    "print(f\"Qtd. registros encontrados no target: {qtd_conta_encontrados}. \")\n",
    "print()\n",
    "print(\"Comparação de 20 registros aleatórios\")\n",
    "df_conta_validacao.sample(False, 0.1, seed=0).limit(20).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f92e44",
   "metadata": {},
   "source": [
    "**Arquivo cartão**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "59a9953c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validação da quantidade de registros: CSV x Target (Data Lake)\n",
      "\n",
      "Qtd. registros do arquivo csv: 10001.\n",
      "Qtd. registros encontrados no target: 10001. \n",
      "\n",
      "Comparação de 20 registros aleatórios\n",
      "+------+------+----------------+----------------+----------------+----------------+------------+------------+----------------+----------------+\n",
      "|id_csv|id_tgt|  num_cartao_csv|  num_cartao_tgt|nom_impresso_csv|nom_impresso_tgt|id_conta_csv|id_conta_tgt|id_associado_csv|id_associado_tgt|\n",
      "+------+------+----------------+----------------+----------------+----------------+------------+------------+----------------+----------------+\n",
      "|   471|   471|5803004704713590|5803004704713590| MIRELLA BARBOSA| MIRELLA BARBOSA|         471|         471|             471|             471|\n",
      "|  1591|  1591|7542003915911574|7542003915911574|     LUCCA COSTA|     LUCCA COSTA|        1591|        1591|            1591|            1591|\n",
      "|  2659|  2659|4877006326597672|4877006326597672|   DANIELA CUNHA|   DANIELA CUNHA|        2659|        2659|            2659|            2659|\n",
      "|  4900|  4900|2674005249002878|2674005249002878|    YURI ALMEIDA|    YURI ALMEIDA|        4900|        4900|            4900|            4900|\n",
      "|  7982|  7982|5888003179825010|5888003179825010|   MARIANA PINTO|   MARIANA PINTO|        7982|        7982|            7982|            7982|\n",
      "|   243|   243|4745003402435264|4745003402435264|    LUCAS NOVAES|    LUCAS NOVAES|         243|         243|             243|             243|\n",
      "|   392|   392|7516002203922622|7516002203922622|      PAULO ROSA|      PAULO ROSA|         392|         392|             392|              -1|\n",
      "|  1127|  1127|9893005011276433|9893005011276433|    ANA CAROLINA|    ANA CAROLINA|        1127|        1127|            1127|            1127|\n",
      "|  2811|  2811|8598006628116253|8598006628116253|    HELENA PORTO|    HELENA PORTO|        2811|        2811|            2811|            2811|\n",
      "|  6623|  6623|2702004266238648|2702004266238648|     JOÃO MIGUEL|     JOÃO MIGUEL|        6623|        6623|            6623|            6623|\n",
      "|  1650|  1650|2352005716504428|2352005716504428|       ANA JÚLIA|       ANA JÚLIA|        1650|        1650|            1650|            1650|\n",
      "|  3488|  3488|5796007034886987|5796007034886987|    MIGUEL SOUZA|    MIGUEL SOUZA|        3488|        3488|            3488|            3488|\n",
      "|  6482|  6482|2440003464828713|2440003464828713|    ENZO GABRIEL|    ENZO GABRIEL|        6482|        6482|            6482|            6482|\n",
      "|  6622|  6622|6723005566221778|6723005566221778| MARIANA CARDOSO| MARIANA CARDOSO|        6622|        6622|            6622|            6622|\n",
      "|  7879|  7879|4764001878794303|4764001878794303| NICOLAS BARBOSA| NICOLAS BARBOSA|        7879|        7879|            7879|            7879|\n",
      "|  8407|  8407|4300004584077301|4300004584077301|       DAVI LUIZ|       DAVI LUIZ|        8407|        8407|            8407|            8407|\n",
      "|  2721|  2721|2277003927218254|2277003927218254|   HELOÍSA VIANA|   HELOÍSA VIANA|        2721|        2721|            2721|            2721|\n",
      "|  3796|  3796|4517003237964041|4517003237964041|    DANIEL PIRES|    DANIEL PIRES|        3796|        3796|            3796|            3796|\n",
      "|  4078|  4078|2421002340787656|2421002340787656|      SRTA. LAÍS|      SRTA. LAÍS|        4078|        4078|            4078|            4078|\n",
      "|  4364|  4364|1486007043646939|1486007043646939|      NINA ROCHA|      NINA ROCHA|        4364|        4364|            4364|            4364|\n",
      "+------+------+----------------+----------------+----------------+----------------+------------+------------+----------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_cartao_validacao=spark.sql('''\n",
    "    select \n",
    "          csv.id as id_csv,\n",
    "          tgt.id as id_tgt,\n",
    "          csv.num_cartao as num_cartao_csv,\n",
    "          tgt.num_cartao as num_cartao_tgt,\n",
    "          csv.nom_impresso as nom_impresso_csv,\n",
    "          tgt.nom_impresso as nom_impresso_tgt,\n",
    "          csv.id_conta as id_conta_csv,\n",
    "          tgt.id_conta as id_conta_tgt,\n",
    "          csv.id_associado as id_associado_csv,\n",
    "          tgt.id_associado as id_associado_tgt\n",
    "        \n",
    "    from cartao_csv csv\n",
    "    \n",
    "    inner join cartao_tgt tgt\n",
    "    on tgt.id=csv.id\n",
    "''')\n",
    "\n",
    "qtd_cartao_csv=df_cartao_csv.count()\n",
    "qtd_cartao_encontrados=df_cartao_validacao.count()\n",
    "\n",
    "print(\"Validação da quantidade de registros: CSV x Target (Data Lake)\")\n",
    "print()\n",
    "print(f\"Qtd. registros do arquivo csv: {qtd_cartao_csv}.\")\n",
    "print(f\"Qtd. registros encontrados no target: {qtd_cartao_encontrados}. \")\n",
    "print()\n",
    "print(\"Comparação de 20 registros aleatórios\")\n",
    "df_cartao_validacao.sample(False, 0.1, seed=0).limit(20).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148ede10",
   "metadata": {},
   "source": [
    "**Arquivo movimento**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bdde053b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validação da quantidade de registros: CSV x Target (Data Lake)\n",
      "\n",
      "Qtd. registros do arquivo csv: 1729322.\n",
      "Qtd. registros encontrados no target: 1729322. \n",
      "\n",
      "Comparação de 20 registros aleatórios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 43:====================================================> (196 + 1) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+-----------------+-----------------+-----------------+-----------------+------------------+------------------+-------------+-------------+\n",
      "|id_csv|id_tgt|vlr_transacao_csv|vlr_transacao_tgt|des_transacao_csv|des_transacao_tgt|data_movimento_csv|data_movimento_tgt|id_cartao_csv|id_cartao_tgt|\n",
      "+------+------+-----------------+-----------------+-----------------+-----------------+------------------+------------------+-------------+-------------+\n",
      "|   471|   471|           430.63|           430.63|            Roupa|            Roupa|        2021-11-04|        2021-11-04|            3|            3|\n",
      "|  1591|  1591|           360.14|           360.14|     Supermercado|     Supermercado|        2021-10-07|        2021-10-07|            9|            9|\n",
      "|  2659|  2659|           268.73|           268.73|      Restaurante|      Restaurante|        2019-04-19|        2019-04-19|           18|           18|\n",
      "|  4900|  4900|           153.90|           153.90|         Farmacia|         Farmacia|        2021-07-05|        2021-07-05|           31|           31|\n",
      "|  7982|  7982|           135.06|           135.06|     Supermercado|     Supermercado|        2022-07-22|        2022-07-22|           48|           48|\n",
      "| 10206| 10206|           300.48|           300.48|            Roupa|            Roupa|        2021-05-24|        2021-05-24|           65|           65|\n",
      "| 10362| 10362|           213.24|           213.24|Posto combustivel|Posto combustivel|        2022-01-23|        2022-01-23|           66|           66|\n",
      "| 11858| 11858|           261.50|           261.50|     Supermercado|     Supermercado|        2021-10-07|        2021-10-07|           75|           75|\n",
      "| 15447| 15447|           192.21|           192.21|            Roupa|            Roupa|        2019-11-21|        2019-11-21|           99|           99|\n",
      "| 17679| 17679|           415.31|           415.31|         Pet shop|         Pet shop|        2020-09-28|        2020-09-28|          109|          109|\n",
      "| 21220| 21220|            52.30|            52.30|         Pet shop|         Pet shop|        2021-10-12|        2021-10-12|          131|          131|\n",
      "| 24171| 24171|           110.17|           110.17|     Supermercado|     Supermercado|        2022-10-14|        2022-10-14|          145|          145|\n",
      "| 26708| 26708|           374.34|           374.34|      Restaurante|      Restaurante|        2020-01-31|        2020-01-31|          158|          158|\n",
      "| 27484| 27484|            51.26|            51.26|     Supermercado|     Supermercado|        2020-02-15|        2020-02-15|          163|          163|\n",
      "| 28124| 28124|           654.13|           654.13|            Roupa|            Roupa|        2022-05-29|        2022-05-29|          165|          165|\n",
      "| 28577| 28577|           971.47|           971.47|     Supermercado|     Supermercado|        2019-10-09|        2019-10-09|          168|          168|\n",
      "| 31367| 31367|           134.50|           134.50|      Restaurante|      Restaurante|        2019-03-25|        2019-03-25|          183|          183|\n",
      "| 32445| 32445|           477.31|           477.31|         Farmacia|         Farmacia|        2022-09-21|        2022-09-21|          187|          187|\n",
      "| 32855| 32855|           167.84|           167.84|         Pet shop|         Pet shop|        2020-08-17|        2020-08-17|          189|          189|\n",
      "| 33569| 33569|           316.32|           316.32|         Farmacia|         Farmacia|        2021-12-11|        2021-12-11|          193|          193|\n",
      "+------+------+-----------------+-----------------+-----------------+-----------------+------------------+------------------+-------------+-------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_movimento_validacao=spark.sql('''\n",
    "    select \n",
    "        csv.id as id_csv,\n",
    "        tgt.id as id_tgt,\n",
    "        csv.vlr_transacao as vlr_transacao_csv,\n",
    "        tgt.vlr_transacao as vlr_transacao_tgt,\n",
    "        csv.des_transacao as des_transacao_csv,\n",
    "        tgt.des_transacao as des_transacao_tgt,\n",
    "        csv.data_movimento as data_movimento_csv,\n",
    "        tgt.data_movimento as data_movimento_tgt,\n",
    "        csv.id_cartao as id_cartao_csv,\n",
    "        tgt.id_cartao as id_cartao_tgt\n",
    "        \n",
    "    from movimento_csv csv\n",
    "    \n",
    "    inner join movimento_tgt tgt\n",
    "    on tgt.id=csv.id\n",
    "''')\n",
    "\n",
    "qtd_movimento_csv=df_movimento_csv.count()\n",
    "qtd_movimento_encontrados=df_movimento_validacao.count()\n",
    "\n",
    "print(\"Validação da quantidade de registros: CSV x Target (Data Lake)\")\n",
    "print()\n",
    "print(f\"Qtd. registros do arquivo csv: {qtd_movimento_csv}.\")\n",
    "print(f\"Qtd. registros encontrados no target: {qtd_movimento_encontrados}. \")\n",
    "print()\n",
    "print(\"Comparação de 20 registros aleatórios\")\n",
    "df_movimento_validacao.sample(False, 0.1, seed=0).limit(20).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3538c220",
   "metadata": {},
   "source": [
    "**Arquivo encerramento conta**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a0a40a98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validação da quantidade de registros: CSV x Target (Data Lake)\n",
      "\n",
      "Qtd. registros do arquivo csv: 10000.\n",
      "Qtd. registros encontrados no target: 10000. \n",
      "\n",
      "Comparação de 20 registros aleatórios\n",
      "+------+------+----------------+----------------+---------------------+---------------------+\n",
      "|id_csv|id_tgt|data_criacao_csv|data_criacao_tgt|data_encerramento_csv|data_encerramento_tgt|\n",
      "+------+------+----------------+----------------+---------------------+---------------------+\n",
      "|   471|   471|      2018-04-12|      2018-04-12|                 null|                 null|\n",
      "|  1591|  1591|      2021-02-01|      2021-02-01|                 null|                 null|\n",
      "|  2659|  2659|      2018-05-26|      2018-05-26|                 null|                 null|\n",
      "|  4900|  4900|      2021-10-24|      2021-10-24|                 null|                 null|\n",
      "|  7982|  7982|      2021-07-14|      2021-07-14|           2022-10-18|           2022-10-18|\n",
      "|   243|   243|      2022-02-09|      2022-02-09|                 null|                 null|\n",
      "|   392|   392|      2020-07-30|      2020-07-30|                 null|                 null|\n",
      "|  1127|  1127|      2020-03-16|      2020-03-16|                 null|                 null|\n",
      "|  2811|  2811|      2020-04-01|      2020-04-01|                 null|                 null|\n",
      "|  6623|  6623|      2020-08-05|      2020-08-05|                 null|                 null|\n",
      "|  1699|  1699|      2018-01-21|      2018-01-21|                 null|                 null|\n",
      "|  3704|  3704|      2021-06-09|      2021-06-09|                 null|                 null|\n",
      "|  6559|  6559|      2022-07-27|      2022-07-27|                 null|                 null|\n",
      "|  6825|  6825|      2021-04-02|      2021-04-02|                 null|                 null|\n",
      "|  8222|  8222|      2022-10-05|      2022-10-05|                 null|                 null|\n",
      "|  8924|  8924|      2022-03-12|      2022-03-12|                 null|                 null|\n",
      "|  2748|  2748|      2019-05-03|      2019-05-03|                 null|                 null|\n",
      "|  3876|  3876|      2022-08-09|      2022-08-09|                 null|                 null|\n",
      "|  4186|  4186|      2022-04-20|      2022-04-20|                 null|                 null|\n",
      "|  4684|  4684|      2019-10-07|      2019-10-07|                 null|                 null|\n",
      "+------+------+----------------+----------------+---------------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_encerramento_conta_validacao=spark.sql('''\n",
    "    select \n",
    "        csv.id as id_csv,\n",
    "        tgt.id as id_tgt,\n",
    "        csv.data_criacao as data_criacao_csv,\n",
    "        tgt.data_criacao as data_criacao_tgt,\n",
    "        csv.data_encerramento as data_encerramento_csv,\n",
    "        tgt.data_encerramento as data_encerramento_tgt\n",
    "        \n",
    "    from encerramento_conta_csv csv\n",
    "    \n",
    "    inner join encerramento_conta_tgt tgt\n",
    "    on tgt.id=csv.id\n",
    "''')\n",
    "\n",
    "qtd_encerramento_conta_csv=df_encerramento_csv.count()\n",
    "qtd_encerramento_conta_encontrados=df_encerramento_conta_validacao.count()\n",
    "\n",
    "print(\"Validação da quantidade de registros: CSV x Target (Data Lake)\")\n",
    "print()\n",
    "print(f\"Qtd. registros do arquivo csv: {qtd_encerramento_conta_csv}.\")\n",
    "print(f\"Qtd. registros encontrados no target: {qtd_encerramento_conta_encontrados}. \")\n",
    "print()\n",
    "print(\"Comparação de 20 registros aleatórios\")\n",
    "df_encerramento_conta_validacao.sample(False, 0.1, seed=0).limit(20).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa61e49",
   "metadata": {},
   "source": [
    "**Arquivo fatura**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5b2d0fa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validação da quantidade de registros: CSV x Target (Data Lake)\n",
      "\n",
      "Qtd. registros do arquivo csv: 303410.\n",
      "Qtd. registros encontrados no target: 303410. \n",
      "\n",
      "Comparação de 20 registros aleatórios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 58:>                                                         (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+--------------------------+--------------------------+--------------+--------------+-------------------------+-------------------------+------------------------+------------------------+-------------+-------------+\n",
      "|id_csv|id_tgt|data_vencimento_fatura_csv|data_vencimento_fatura_tgt|vlr_fatura_csv|vlr_fatura_tgt|data_pagamento_fatura_csv|data_pagamento_fatura_tgt|qtd_dias_atraso_pgto_csv|qtd_dias_atraso_pgto_tgt|id_cartao_csv|id_cartao_tgt|\n",
      "+------+------+--------------------------+--------------------------+--------------+--------------+-------------------------+-------------------------+------------------------+------------------------+-------------+-------------+\n",
      "|   471|   471|                2022-07-15|                2022-07-15|          0.00|          0.00|               2022-07-15|               2022-07-15|                       0|                       0|           17|           17|\n",
      "|  1591|  1591|                2020-11-15|                2020-11-15|       1532.04|       1532.04|               2020-11-12|               2020-11-12|                       0|                       0|           56|           56|\n",
      "|  2659|  2659|                2021-04-15|                2021-04-15|          0.00|          0.00|               2021-04-15|               2021-04-15|                       0|                       0|           96|           96|\n",
      "|  4900|  4900|                2021-09-15|                2021-09-15|        575.34|        575.34|               2021-09-15|               2021-09-15|                       0|                       0|          161|          161|\n",
      "|  7982|  7982|                2020-01-15|                2020-01-15|        583.32|        583.32|               2020-01-15|               2020-01-15|                       0|                       0|          261|          261|\n",
      "| 10206| 10206|                2020-02-15|                2020-02-15|        262.09|        262.09|               2020-02-12|               2020-02-12|                       0|                       0|          336|          336|\n",
      "| 10362| 10362|                2019-06-15|                2019-06-15|        170.07|        170.07|               2019-06-13|               2019-06-13|                       0|                       0|          341|          341|\n",
      "| 11858| 11858|                2021-06-15|                2021-06-15|       2775.54|       2775.54|               2021-06-14|               2021-06-14|                       0|                       0|          390|          390|\n",
      "| 15447| 15447|                2020-04-15|                2020-04-15|        153.96|        153.96|               2020-04-11|               2020-04-11|                       0|                       0|          501|          501|\n",
      "| 17679| 17679|                2022-08-15|                2022-08-15|       1537.50|       1537.50|               2022-08-12|               2022-08-12|                       0|                       0|          581|          581|\n",
      "| 21220| 21220|                2020-12-15|                2020-12-15|       2241.38|       2241.38|               2020-12-12|               2020-12-12|                       0|                       0|          690|          690|\n",
      "| 24171| 24171|                2021-04-15|                2021-04-15|       2847.57|       2847.57|               2021-04-10|               2021-04-10|                       0|                       0|          781|          781|\n",
      "| 26708| 26708|                2021-07-15|                2021-07-15|       1527.93|       1527.93|               2021-07-14|               2021-07-14|                       0|                       0|          868|          868|\n",
      "| 27484| 27484|                2020-06-15|                2020-06-15|        771.25|        771.25|               2020-06-15|               2020-06-15|                       0|                       0|          891|          891|\n",
      "| 28124| 28124|                2022-12-15|                2022-12-15|        960.57|        960.57|               2022-12-12|               2022-12-12|                       0|                       0|          911|          911|\n",
      "| 28577| 28577|                2021-04-15|                2021-04-15|       2592.95|       2592.95|               2021-04-13|               2021-04-13|                       0|                       0|          924|          924|\n",
      "| 31367| 31367|                2022-06-15|                2022-06-15|       1615.79|       1615.79|               2022-06-13|               2022-06-13|                       0|                       0|         1014|         1014|\n",
      "| 32445| 32445|                2020-06-15|                2020-06-15|        482.22|        482.22|               2020-06-13|               2020-06-13|                       0|                       0|         1052|         1052|\n",
      "| 32855| 32855|                2021-10-15|                2021-10-15|        756.21|        756.21|               2021-10-15|               2021-10-15|                       0|                       0|         1063|         1063|\n",
      "| 33569| 33569|                2019-06-15|                2019-06-15|        331.93|        331.93|               2019-06-15|               2019-06-15|                       0|                       0|         1090|         1090|\n",
      "+------+------+--------------------------+--------------------------+--------------+--------------+-------------------------+-------------------------+------------------------+------------------------+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_fatura_validacao=spark.sql('''\n",
    "    select \n",
    "        csv.id as id_csv,\n",
    "        tgt.id as id_tgt,\n",
    "        csv.data_vencimento_fatura as data_vencimento_fatura_csv,\n",
    "        tgt.data_vencimento_fatura as data_vencimento_fatura_tgt,\n",
    "        csv.vlr_fatura as vlr_fatura_csv,\n",
    "        tgt.vlr_fatura as vlr_fatura_tgt,\n",
    "        csv.data_pagamento_fatura as data_pagamento_fatura_csv,\n",
    "        tgt.data_pagamento_fatura as data_pagamento_fatura_tgt,\n",
    "        csv.qtd_dias_atraso_pgto as qtd_dias_atraso_pgto_csv,\n",
    "        tgt.qtd_dias_atraso_pgto as qtd_dias_atraso_pgto_tgt,\n",
    "        csv.id_cartao as id_cartao_csv,\n",
    "        tgt.id_cartao as id_cartao_tgt\n",
    "        \n",
    "    from fatura_csv csv\n",
    "    \n",
    "    inner join fatura_tgt tgt\n",
    "    on tgt.id=csv.id\n",
    "''')\n",
    "\n",
    "qtd_fatura_csv=df_fatura_csv.count()\n",
    "qtd_fatura_encontrados=df_fatura_validacao.count()\n",
    "\n",
    "print(\"Validação da quantidade de registros: CSV x Target (Data Lake)\")\n",
    "print()\n",
    "print(f\"Qtd. registros do arquivo csv: {qtd_fatura_csv}.\")\n",
    "print(f\"Qtd. registros encontrados no target: {qtd_fatura_encontrados}. \")\n",
    "print()\n",
    "print(\"Comparação de 20 registros aleatórios\")\n",
    "df_fatura_validacao.sample(False, 0.1, seed=0).limit(20).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5c3e26",
   "metadata": {},
   "source": [
    "### Teste integrado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d680999",
   "metadata": {},
   "source": [
    "O objetivo do teste integrado é validar se existe algum problema na integridade de relacionamento entre os dados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295ab6e4",
   "metadata": {},
   "source": [
    "**Validar a integridade de relacionamento entre as tabelas Conta e Associado**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "63dccd4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validação da quantidade de registros: CSV x Target (Data Lake)\n",
      "\n",
      "Qtd. registros não encontrados por problemas de integridade: 0.\n",
      "Qtd. registros tratados com '-1': 50\n",
      "\n",
      "20 registros aleatórios com tratamento de: -1\n",
      "+----+--------------+------------+------------+\n",
      "|  id|          tipo|data_criacao|id_associado|\n",
      "+----+--------------+------------+------------+\n",
      "| 392|Conta Corrente|  2020-07-30|          -1|\n",
      "|6011|Conta Corrente|  2020-07-20|          -1|\n",
      "|1290|Conta Corrente|  2018-10-10|          -1|\n",
      "|4823|Conta Corrente|  2019-08-19|          -1|\n",
      "|4481|Conta Corrente|  2022-12-09|          -1|\n",
      "|7519|Conta Corrente|  2019-12-24|          -1|\n",
      "|1266|Conta Corrente|  2021-01-28|          -1|\n",
      "|8818|Conta Corrente|  2019-03-07|          -1|\n",
      "|9176|Conta Corrente|  2022-10-07|          -1|\n",
      "|4352|Conta Corrente|  2018-01-24|          -1|\n",
      "|7581|Conta Corrente|  2019-12-20|          -1|\n",
      "|7158|Conta Corrente|  2018-02-22|          -1|\n",
      "|6748|Conta Corrente|  2021-12-28|          -1|\n",
      "|9350|Conta Corrente|  2021-04-02|          -1|\n",
      "|1967|Conta Corrente|  2018-06-19|          -1|\n",
      "|6312|Conta Corrente|  2021-07-03|          -1|\n",
      "|4071|Conta Corrente|  2018-08-27|          -1|\n",
      "|2674|Conta Corrente|  2018-12-16|          -1|\n",
      "|2258|Conta Corrente|  2020-06-23|          -1|\n",
      "|2827|Conta Corrente|  2021-09-24|          -1|\n",
      "+----+--------------+------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_nao_encontrado=spark.sql('''\n",
    "    select tgt1.id_associado\n",
    "    \n",
    "    from conta_tgt tgt1\n",
    "    \n",
    "    left join associado_tgt tgt2\n",
    "    on tgt2.id=tgt1.id_associado\n",
    "    \n",
    "    where tgt2.id is null\n",
    "''')\n",
    "\n",
    "qtd_nao_encontrado=df_nao_encontrado.count()\n",
    "\n",
    "df_tratado=spark.sql('''\n",
    "    select tgt1.*\n",
    "    \n",
    "    from conta_tgt tgt1\n",
    "    \n",
    "    where tgt1.id_associado=-1\n",
    "    and tgt1.id<>-1\n",
    "''')\n",
    "\n",
    "qtd_tratados=df_tratado.count()\n",
    "\n",
    "print(\"Validação da quantidade de registros: CSV x Target (Data Lake)\")\n",
    "print()\n",
    "print(f\"Qtd. registros não encontrados por problemas de integridade: {qtd_nao_encontrado}.\")\n",
    "print(f\"Qtd. registros tratados com '-1': {qtd_tratados}\")\n",
    "print()\n",
    "print(\"20 registros aleatórios com tratamento de: -1\")\n",
    "df_tratado.sample(False, 0.99, seed=0).limit(20).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89466fcc",
   "metadata": {},
   "source": [
    "**Validar a integridade de relacionamento entre as tabelas Cartao, Conta e Associado**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d33efd15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 75:================================================>     (181 + 1) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validação da quantidade de registros: CSV x Target (Data Lake)\n",
      "\n",
      "Qtd. registros não encontrados por problemas de integridade (Associado): 0.\n",
      "Qtd. registros não encontrados por problemas de integridade (Conta): 0.\n",
      "\n",
      "Qtd. registros tratados com '-1' (Associado): 50\n",
      "20 registros aleatórios com tratamento de: -1 (Associado)\n",
      "+----+----------------+---------------+--------+------------+\n",
      "|  id|      num_cartao|   nom_impresso|id_conta|id_associado|\n",
      "+----+----------------+---------------+--------+------------+\n",
      "| 392|7516002203922622|     PAULO ROSA|     392|          -1|\n",
      "|6011|7080006960112516|   JOÃO GABRIEL|    6011|          -1|\n",
      "|1290|3562005212902320|  MIRELLA ROCHA|    1290|          -1|\n",
      "|4823|1721005648239215|    JOÃO MIGUEL|    4823|          -1|\n",
      "|4481|2690003944816094|  VICENTE RAMOS|    4481|          -1|\n",
      "|7519|2207002875190032|   JÚLIA CASTRO|    7519|          -1|\n",
      "|1266|5979005712668585|VALENTINA CUNHA|    1266|          -1|\n",
      "|8818|3039004288185454|  ANDRÉ PEREIRA|    8818|          -1|\n",
      "|9176|8326004491766570|     DAVI LUCCA|    9176|          -1|\n",
      "|4352|8175005043528487|  BRUNO REZENDE|    4352|          -1|\n",
      "|7581|2276004575812746| MARIA FERNANDA|    7581|          -1|\n",
      "|7158|3088004571584330|    YASMIN DIAS|    7158|          -1|\n",
      "|6748|7121005067482756|      LUIGI PAZ|    6748|          -1|\n",
      "|9350|4302006693504358|     DANIEL PAZ|    9350|          -1|\n",
      "|1967|7614004019675322|    BRUNO PIRES|    1967|          -1|\n",
      "|6312|3344004963126242|  ANTÔNIO ROCHA|    6312|          -1|\n",
      "|4071|6180003840719159|   ENZO RIBEIRO|    4071|          -1|\n",
      "|2674|8465002526749190| BRENDA RIBEIRO|    2674|          -1|\n",
      "|2258|4298002822586857|   ESTHER SILVA|    2258|          -1|\n",
      "|2827|6554006228271468|  YASMIN MORAES|    2827|          -1|\n",
      "+----+----------------+---------------+--------+------------+\n",
      "\n",
      "\n",
      "Qtd. registros tratados com '-1'(Conta): 0\n",
      "20 registros aleatórios com tratamento de: -1 (Conta)\n",
      "+---+----------+------------+--------+------------+\n",
      "| id|num_cartao|nom_impresso|id_conta|id_associado|\n",
      "+---+----------+------------+--------+------------+\n",
      "+---+----------+------------+--------+------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_nao_encontrado_1=spark.sql('''\n",
    "    select tgt1.id_associado\n",
    "    \n",
    "    from cartao_tgt tgt1\n",
    "    \n",
    "    left join associado_tgt tgt2\n",
    "    on tgt2.id=tgt1.id_associado\n",
    "    \n",
    "    where tgt2.id is null\n",
    "''')\n",
    "\n",
    "qtd_nao_encontrado_1=df_nao_encontrado_1.count()\n",
    "\n",
    "df_nao_encontrado_2=spark.sql('''\n",
    "    select tgt1.id_associado\n",
    "    \n",
    "    from cartao_tgt tgt1\n",
    "    \n",
    "    left join conta_tgt tgt2\n",
    "    on tgt2.id=tgt1.id_conta\n",
    "    \n",
    "    where tgt2.id is null\n",
    "''')\n",
    "qtd_nao_encontrado_2=df_nao_encontrado_2.count()\n",
    "\n",
    "df_tratado_1=spark.sql('''\n",
    "    select tgt1.*\n",
    "    \n",
    "    from cartao_tgt tgt1\n",
    "    \n",
    "    where tgt1.id_associado=-1\n",
    "    and tgt1.id<>-1\n",
    "''')\n",
    "\n",
    "qtd_tratados_1=df_tratado_1.count()\n",
    "\n",
    "df_tratado_2=spark.sql('''\n",
    "    select tgt1.*\n",
    "    \n",
    "    from cartao_tgt tgt1\n",
    "    \n",
    "    where tgt1.id_conta=-1\n",
    "    and tgt1.id<>-1\n",
    "''')\n",
    "\n",
    "qtd_tratados_2=df_tratado_2.count()\n",
    "\n",
    "print(\"Validação da quantidade de registros: CSV x Target (Data Lake)\")\n",
    "print()\n",
    "print(f\"Qtd. registros não encontrados por problemas de integridade (Associado): {qtd_nao_encontrado_1}.\")\n",
    "print(f\"Qtd. registros não encontrados por problemas de integridade (Conta): {qtd_nao_encontrado_2}.\")\n",
    "print()\n",
    "print(f\"Qtd. registros tratados com '-1' (Associado): {qtd_tratados_1}\")\n",
    "print(\"20 registros aleatórios com tratamento de: -1 (Associado)\")\n",
    "df_tratado_1.sample(False, 0.99, seed=0).limit(20).show()\n",
    "print()\n",
    "print(f\"Qtd. registros tratados com '-1'(Conta): {qtd_tratados_2}\")\n",
    "print(\"20 registros aleatórios com tratamento de: -1 (Conta)\")\n",
    "df_tratado_2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b661d4",
   "metadata": {},
   "source": [
    "**Validar a integridade de relacionamento entre as tabelas Cartao e Movimento**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "10074561",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 86:============================================>         (163 + 1) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validação da quantidade de registros: CSV x Target (Data Lake)\n",
      "\n",
      "Qtd. registros não encontrados por problemas de integridade: 0.\n",
      "\n",
      "Qtd. registros tratados com '-1': 0\n",
      "20 registros aleatórios com tratamento de: -1\n",
      "+---+-------------+-------------+--------------+---------+\n",
      "| id|vlr_transacao|des_transacao|data_movimento|id_cartao|\n",
      "+---+-------------+-------------+--------------+---------+\n",
      "+---+-------------+-------------+--------------+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 86:====================================================> (195 + 1) / 200]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_nao_encontrado=spark.sql('''\n",
    "    select tgt1.id_cartao\n",
    "    \n",
    "    from movimento_tgt tgt1\n",
    "    \n",
    "    left join cartao_tgt tgt2\n",
    "    on tgt2.id=tgt1.id_cartao\n",
    "    \n",
    "    where tgt2.id is null\n",
    "''')\n",
    "\n",
    "qtd_nao_encontrado=df_nao_encontrado.count()\n",
    "\n",
    "df_tratado=spark.sql('''\n",
    "    select tgt1.*\n",
    "    \n",
    "    from movimento_tgt tgt1\n",
    "    \n",
    "    where tgt1.id_cartao=-1\n",
    "    and tgt1.id<>-1\n",
    "''')\n",
    "\n",
    "qtd_tratados=df_tratado.count()\n",
    "\n",
    "print(\"Validação da quantidade de registros: CSV x Target (Data Lake)\")\n",
    "print()\n",
    "print(f\"Qtd. registros não encontrados por problemas de integridade: {qtd_nao_encontrado}.\")\n",
    "print()\n",
    "print(f\"Qtd. registros tratados com '-1': {qtd_tratados}\")\n",
    "print(\"20 registros aleatórios com tratamento de: -1\")\n",
    "df_tratado.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ea05e9",
   "metadata": {},
   "source": [
    "**Validar a integridade de relacionamento entre as tabelas Encerramento Conta e Conta**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "400ba10f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 93:====================================>                 (135 + 1) / 200]\r",
      "\r",
      "[Stage 93:=================================================>    (184 + 1) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validação da quantidade de registros: CSV x Target (Data Lake)\n",
      "\n",
      "Qtd. registros não encontrados por problemas de integridade: 0.\n",
      "\n",
      "Qtd. registros tratados com '-1': 0\n",
      "20 registros aleatórios com tratamento de: -1\n",
      "+---+------------+-----------------+\n",
      "| id|data_criacao|data_encerramento|\n",
      "+---+------------+-----------------+\n",
      "+---+------------+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_nao_encontrado=spark.sql('''\n",
    "    select tgt1.id\n",
    "    \n",
    "    from encerramento_conta_tgt tgt1\n",
    "    \n",
    "    left join conta_tgt tgt2\n",
    "    on tgt2.id=tgt1.id\n",
    "    \n",
    "    where tgt2.id is null\n",
    "''')\n",
    "\n",
    "qtd_nao_encontrado=df_nao_encontrado.count()\n",
    "\n",
    "df_tratado=spark.sql('''\n",
    "    select tgt1.*\n",
    "    \n",
    "    from encerramento_conta_tgt tgt1\n",
    "    \n",
    "    where tgt1.id=-1\n",
    "    and tgt1.id<>-1\n",
    "''')\n",
    "\n",
    "qtd_tratados=df_tratado.count()\n",
    "\n",
    "print(\"Validação da quantidade de registros: CSV x Target (Data Lake)\")\n",
    "print()\n",
    "print(f\"Qtd. registros não encontrados por problemas de integridade: {qtd_nao_encontrado}.\")\n",
    "print()\n",
    "print(f\"Qtd. registros tratados com '-1': {qtd_tratados}\")\n",
    "print(\"20 registros aleatórios com tratamento de: -1\")\n",
    "df_tratado.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657bf829",
   "metadata": {},
   "source": [
    "**Validar a integridade de relacionamento entre as tabelas Fatura e Cartao**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "aace73ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 99:=============================================>        (170 + 1) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validação da quantidade de registros: CSV x Target (Data Lake)\n",
      "\n",
      "Qtd. registros não encontrados por problemas de integridade: 0.\n",
      "\n",
      "Qtd. registros tratados com '-1': 0\n",
      "20 registros aleatórios com tratamento de: -1\n",
      "+---+----------------------+----------+---------------------+--------------------+---------+\n",
      "| id|data_vencimento_fatura|vlr_fatura|data_pagamento_fatura|qtd_dias_atraso_pgto|id_cartao|\n",
      "+---+----------------------+----------+---------------------+--------------------+---------+\n",
      "+---+----------------------+----------+---------------------+--------------------+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 99:===================================================>  (190 + 1) / 200]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_nao_encontrado=spark.sql('''\n",
    "    select tgt1.id\n",
    "    \n",
    "    from fatura_tgt tgt1\n",
    "    \n",
    "    left join fatura_tgt tgt2\n",
    "    on tgt2.id=tgt1.id\n",
    "    \n",
    "    where tgt2.id is null\n",
    "''')\n",
    "\n",
    "qtd_nao_encontrado=df_nao_encontrado.count()\n",
    "\n",
    "df_tratado=spark.sql('''\n",
    "    select tgt1.*\n",
    "    \n",
    "    from fatura_tgt tgt1\n",
    "    \n",
    "    where tgt1.id=-1\n",
    "    and tgt1.id<>-1\n",
    "''')\n",
    "\n",
    "qtd_tratados=df_tratado.count()\n",
    "\n",
    "print(\"Validação da quantidade de registros: CSV x Target (Data Lake)\")\n",
    "print()\n",
    "print(f\"Qtd. registros não encontrados por problemas de integridade: {qtd_nao_encontrado}.\")\n",
    "print()\n",
    "print(f\"Qtd. registros tratados com '-1': {qtd_tratados}\")\n",
    "print(\"20 registros aleatórios com tratamento de: -1\")\n",
    "df_tratado.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2cbdd4a",
   "metadata": {},
   "source": [
    "### Resultado dos testes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201967e2",
   "metadata": {},
   "source": [
    "**Teste unitário**\n",
    "\n",
    "Não foi identificado nenhuma irregulariedade em relação aos dados.\n",
    "Todos os dados de origem foram armazenados corretamente no Data Lake.\n",
    "\n",
    "\n",
    "**Teste integrado**\n",
    "\n",
    "Durante os testes, foi identificado irregularidades nos dados dos associados.\n",
    "\n",
    "Durante os testes das informações de *conta* e *cartao*, foi identificado que existem 50 registros faltantes na tabela de associados.\n",
    "\n",
    "O processo de carga tratou a carga corretamente preenchendo com o valor **-1** na ausência dos associados.\n",
    "\n",
    "**Simulação de Dados - Dados gerados para ocasionar o problema de integridade dos dados**\n",
    "\n",
    "As vezes podem ocorrer erros nos sistemas transacionais que geram inconsistências nos dados.\n",
    "\n",
    "Essas inconsistências afetam diretamente os dados no Data Lake. Através dos processos de carga, podemos contornar alguns problemas de integridade relacional.\n",
    "\n",
    "Para demonstrar melhor o processo de carga, foi excluído propositalmente 50 associados do arquivo de associados, deixando o ambiente mais próximo da realidade que encontramos nas organizações."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661f9b29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
