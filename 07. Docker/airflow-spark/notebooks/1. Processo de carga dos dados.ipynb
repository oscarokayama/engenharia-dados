{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56bc2e94",
   "metadata": {},
   "source": [
    "## 1. Processo de carga dos dados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196f7589",
   "metadata": {},
   "source": [
    "A arquitetura do processo de carga dos dados escolhido será o modelo ETL.\n",
    "\n",
    "O framework escolhido para o processamento dos dados será em PySpark."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c59e168",
   "metadata": {},
   "source": [
    "**Motivo de não desenvolver o projeto no modelo ELT**\n",
    "\n",
    "No cenário atual, onde os dados relacionais são armazenados em um banco de dados relacional como o postgres, talvez o modelo ideal seria o ELT, onde extrairíamos os dados em CSV, carregaríamos os dados diretamente no postgres utilizando os utilitários do postgres e todo o tratamento seria dentro do postgres.\n",
    "\n",
    "A escolha do modelo ETL, é para exercitar o processamento de dados utilizando o motor de processamento do **Spark**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6b029d",
   "metadata": {},
   "source": [
    "**Importação de bibliotecas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a49bf68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_unixtime, col, to_timestamp, coalesce\n",
    "from pyspark.sql.types import StringType, IntegerType, LongType, DecimalType, DateType"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58c1ee0",
   "metadata": {},
   "source": [
    "**Variaveis do projeto**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5eb4fa0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Diretorio dos arquivos csv\n",
    "v_diretorio_csv='/home/jovyan/work/csv/'\n",
    "\n",
    "#Diretorio de export do arquivo de flatfile\n",
    "v_diretorio_export='/home/jovyan/work/export'\n",
    "\n",
    "#Variaveis de conexao com postgres\n",
    "v_caminho_jar_postgres='/home/jovyan/work/jars/postgresql-9.4.1207.jar'\n",
    "v_url_jdbc='jdbc:postgresql://postgres/airflow'\n",
    "v_user_jdbc='airflow'\n",
    "v_pass_jdbc='airflow'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a62882",
   "metadata": {},
   "source": [
    "**Criando sessao e contexto**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce28e957",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.1.2-bin-hadoop3.2/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "22/12/28 00:35:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .master('local')\n",
    "         .appName('load-postgres')\n",
    "         # Add postgres jar\n",
    "         .config('spark.driver.extraClassPath', v_caminho_jar_postgres)\n",
    "         .getOrCreate())\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac95287",
   "metadata": {},
   "source": [
    "**Lendo arquivo csv, criando dataframe spark, formatando e criando views**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83fdfaee",
   "metadata": {},
   "source": [
    "Essa fase do processo, carrega os dados dos arquivos csv em dataframes, formata os campos e cria views para posteriormente serem utilizados na fase de tratamento dos dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8db463dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataframe Associado\n",
    "df_associado_csv = (\n",
    "    spark.read\n",
    "    .format('csv')\n",
    "    .option('header', True)\n",
    "    .option('delimiter', ';')\n",
    "    .load(v_diretorio_csv + 'associado.csv')\n",
    ")\n",
    "\n",
    "#Definindo o tipo da coluna\n",
    "df_associado_csv_fmt = (\n",
    "    df_associado_csv\n",
    "    .withColumn('id', col('id').cast(IntegerType()))\n",
    "    .withColumn('idade', col('idade').cast(IntegerType()))\n",
    ")\n",
    "\n",
    "#Criando view do dataframe\n",
    "df_associado_csv_fmt.createOrReplaceTempView('associado')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27c15f34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+---------+-----+--------------------+\n",
      "| id|     nome|sobrenome|idade|               email|\n",
      "+---+---------+---------+-----+--------------------+\n",
      "|  1|   Alícia|  Cardoso|   29|alícia.cardoso@ho...|\n",
      "|  2|  Mirella|    Moura|   25|mirella.moura@gma...|\n",
      "|  3|  Rodrigo|Fernandes|   54|rodrigo.fernandes...|\n",
      "|  4|   Rebeca|  Cardoso|   59|rebeca.cardoso@te...|\n",
      "|  5|     Raul|   Barros|   51|raul.barros@yahoo...|\n",
      "|  6|    Julia|    Nunes|   38|julia.nunes@yahoo...|\n",
      "|  7|     João|   Miguel|   45|joão.miguel@uol.c...|\n",
      "|  8|Francisco|    Gomes|   27|francisco.gomes@h...|\n",
      "|  9| Vinicius|     Lima|   58|vinicius.lima@hot...|\n",
      "| 10|  Cecília|    Souza|   40|cecília.souza@uol...|\n",
      "| 11|      Ana|    Julia|   57|ana.julia@yahoo.c...|\n",
      "| 12|  Anthony|    Neves|   40|anthony.neves@yah...|\n",
      "| 13|    Lucas|    Costa|   34|lucas.costa@hotma...|\n",
      "| 14|      Ana| Teixeira|   66|ana.teixeira@hotm...|\n",
      "| 15|     João|    Lucas|   70|joão.lucas@uol.co...|\n",
      "| 16|    Bruna|      Luz|   69|bruna.luz@hotmail...|\n",
      "| 17|    Vitor|     Hugo|   67|vitor.hugo@hotmai...|\n",
      "| 18|    Sarah|Fernandes|   39|sarah.fernandes@y...|\n",
      "| 19|  Cecília|Rodrigues|   75|cecília.rodrigues...|\n",
      "| 20|   Nathan|     Mota|   42|nathan.mota@yahoo...|\n",
      "+---+---------+---------+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_associado_csv_fmt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62f45170",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataframe Conta\n",
    "df_conta_csv = (\n",
    "    spark.read\n",
    "    .format('csv')\n",
    "    .option('header', True)\n",
    "    .option('delimiter', ';')\n",
    "    .load(v_diretorio_csv + 'conta.csv')\n",
    ")\n",
    "\n",
    "#Definindo o tipo da coluna\n",
    "df_conta_csv_fmt = (\n",
    "    df_conta_csv\n",
    "    .withColumn('id', col('id').cast(IntegerType()))\n",
    "    .withColumn('data_criacao', col('data_criacao').cast(DateType()))\n",
    "    .withColumn('id_associado', col('id_associado').cast(IntegerType()))\n",
    ")\n",
    "\n",
    "#Criando view do dataframe\n",
    "df_conta_csv_fmt.createOrReplaceTempView('conta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67ec4181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+------------+------------+\n",
      "| id|          tipo|data_criacao|id_associado|\n",
      "+---+--------------+------------+------------+\n",
      "|  1|Conta Corrente|  2019-03-28|           1|\n",
      "|  2|Conta Corrente|  2021-04-02|           2|\n",
      "|  3|Conta Corrente|  2019-05-24|           3|\n",
      "|  4|Conta Corrente|  2018-10-22|           4|\n",
      "|  5|Conta Corrente|  2022-11-29|           5|\n",
      "|  6|Conta Corrente|  2018-05-26|           6|\n",
      "|  7|Conta Corrente|  2020-08-23|           7|\n",
      "|  8|Conta Corrente|  2019-02-16|           8|\n",
      "|  9|Conta Corrente|  2021-03-09|           9|\n",
      "| 10|Conta Corrente|  2022-04-09|          10|\n",
      "| 11|Conta Corrente|  2019-10-08|          11|\n",
      "| 12|Conta Corrente|  2022-04-28|          12|\n",
      "| 13|Conta Corrente|  2019-02-15|          13|\n",
      "| 14|Conta Corrente|  2022-08-21|          14|\n",
      "| 15|Conta Corrente|  2022-07-15|          15|\n",
      "| 16|Conta Corrente|  2019-12-27|          16|\n",
      "| 17|Conta Corrente|  2022-07-31|          17|\n",
      "| 18|Conta Corrente|  2018-07-13|          18|\n",
      "| 19|Conta Corrente|  2019-04-14|          19|\n",
      "| 20|Conta Corrente|  2022-12-07|          20|\n",
      "+---+--------------+------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_conta_csv_fmt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e780167",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataframe Cartao\n",
    "df_cartao_csv = (\n",
    "    spark.read\n",
    "    .format('csv')\n",
    "    .option('header', True)\n",
    "    .option('delimiter', ';')\n",
    "    .load(v_diretorio_csv + 'cartao.csv')\n",
    ")\n",
    "\n",
    "#Definindo o tipo da coluna\n",
    "df_cartao_csv_fmt = (\n",
    "    df_cartao_csv\n",
    "    .withColumn('id', col('id').cast(IntegerType()))\n",
    "    .withColumn('id_conta', col('id_conta').cast(IntegerType()))\n",
    "    .withColumn('id_associado', col('id_associado').cast(IntegerType()))\n",
    ")\n",
    "\n",
    "#Criando view do dataframe\n",
    "df_cartao_csv_fmt.createOrReplaceTempView('cartao')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22095102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+-----------------+--------+------------+\n",
      "| id|      num_cartao|     nom_impresso|id_conta|id_associado|\n",
      "+---+----------------+-----------------+--------+------------+\n",
      "|  1|8692002900010397|   ALÍCIA CARDOSO|       1|           1|\n",
      "|  2|1360002500020347|    MIRELLA MOURA|       2|           2|\n",
      "|  3|3935005400035103|RODRIGO FERNANDES|       3|           3|\n",
      "|  4|4371005900041388|   REBECA CARDOSO|       4|           4|\n",
      "|  5|9500005100053578|      RAUL BARROS|       5|           5|\n",
      "|  6|7915003800066514|      JULIA NUNES|       6|           6|\n",
      "|  7|2184004500079616|      JOÃO MIGUEL|       7|           7|\n",
      "|  8|2631002700088038|  FRANCISCO GOMES|       8|           8|\n",
      "|  9|3191005800091087|    VINICIUS LIMA|       9|           9|\n",
      "| 10|9897004000108416|    CECÍLIA SOUZA|      10|          10|\n",
      "| 11|8684005700115334|        ANA JULIA|      11|          11|\n",
      "| 12|8694004000128933|    ANTHONY NEVES|      12|          12|\n",
      "| 13|9950003400138288|      LUCAS COSTA|      13|          13|\n",
      "| 14|4373006600142001|     ANA TEIXEIRA|      14|          14|\n",
      "| 15|6333007000157004|       JOÃO LUCAS|      15|          15|\n",
      "| 16|9080006900166160|        BRUNA LUZ|      16|          16|\n",
      "| 17|6279006700177996|       VITOR HUGO|      17|          17|\n",
      "| 18|5432003900184311|  SARAH FERNANDES|      18|          18|\n",
      "| 19|4667007500196222|CECÍLIA RODRIGUES|      19|          19|\n",
      "| 20|5578004200205193|      NATHAN MOTA|      20|          20|\n",
      "+---+----------------+-----------------+--------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_cartao_csv_fmt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e2efcb9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataframe Movimento\n",
    "df_movimento_csv = (\n",
    "    spark.read\n",
    "    .format(\"csv\")\n",
    "    .option(\"header\", True)\n",
    "    .option(\"delimiter\", \";\")\n",
    "    .load(v_diretorio_csv + \"movimento.csv\")\n",
    ")\n",
    "\n",
    "#Definindo o tipo da coluna\n",
    "df_movimento_csv_fmt = (\n",
    "    df_movimento_csv\n",
    "    .withColumn('id', col('id').cast(IntegerType()))\n",
    "    .withColumn('vlr_transacao', col('vlr_transacao').cast(DecimalType(10,2)))\n",
    "    .withColumn('data_movimento', col('data_movimento').cast(DateType()))\n",
    "    .withColumn('id_cartao', col('id_cartao').cast(IntegerType()))\n",
    ")\n",
    "\n",
    "#Criando view do dataframe\n",
    "df_movimento_csv_fmt.createOrReplaceTempView('movimento')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "94209042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------+-----------------+--------------+---------+\n",
      "|  id|vlr_transacao|    des_transacao|data_movimento|id_cartao|\n",
      "+----+-------------+-----------------+--------------+---------+\n",
      "|4249|        65.80|      Restaurante|    2022-05-11|       26|\n",
      "|4250|        51.64|            Roupa|    2022-05-16|       26|\n",
      "|4251|       398.16|Posto combustivel|    2022-05-21|       26|\n",
      "|4252|        55.99|Posto combustivel|    2022-05-25|       26|\n",
      "|4253|       218.14|         Farmacia|    2022-06-03|       26|\n",
      "|4254|       543.76|      Restaurante|    2022-06-10|       26|\n",
      "|4255|       495.44|      Restaurante|    2022-06-13|       26|\n",
      "|4256|       123.16|         Pet shop|    2022-06-14|       26|\n",
      "|4257|        35.05|            Roupa|    2022-06-17|       26|\n",
      "|4258|        23.29|         Pet shop|    2022-06-22|       26|\n",
      "|4259|       474.36|     Supermercado|    2022-06-23|       26|\n",
      "|4260|        81.47|      Restaurante|    2022-06-25|       26|\n",
      "|4261|       464.14|         Pet shop|    2022-07-02|       26|\n",
      "|4262|       303.69|      Restaurante|    2022-07-04|       26|\n",
      "|4263|       658.54|      Restaurante|    2022-07-10|       26|\n",
      "|4264|       376.01|      Restaurante|    2022-07-12|       26|\n",
      "|4265|       598.44|      Restaurante|    2022-07-13|       26|\n",
      "|4266|        49.41|     Supermercado|    2022-07-25|       26|\n",
      "|4267|       493.84|            Roupa|    2022-07-27|       26|\n",
      "|4268|       499.91|            Roupa|    2022-07-28|       26|\n",
      "+----+-------------+-----------------+--------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_movimento_csv_fmt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f3a0e74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataframe Encerramento\n",
    "df_encerramento_csv = (\n",
    "    spark.read\n",
    "    .format('csv')\n",
    "    .option('header', True)\n",
    "    .option('delimiter', ';')\n",
    "    .load(v_diretorio_csv + 'encerramento_conta.csv')\n",
    ")\n",
    "\n",
    "#Removendo colunas\n",
    "new_df_encerramento_csv=df_encerramento_csv.drop('semente', 'data_parou_comprar', 'dias_sem_compra')\n",
    "\n",
    "#Definindo o tipo da coluna\n",
    "df_encerramento_csv_fmt = (\n",
    "    new_df_encerramento_csv\n",
    "    .withColumn('id', col('id').cast(IntegerType()))\n",
    "    .withColumn('data_criacao', col('data_criacao').cast(DateType()))\n",
    "    .withColumn('data_encerramento', col('data_encerramento').cast(DateType()))\n",
    ")\n",
    "\n",
    "#Criando view do dataframe\n",
    "df_encerramento_csv_fmt.createOrReplaceTempView('encerramento_conta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d9dc0594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+-----------------+\n",
      "| id|data_criacao|data_encerramento|\n",
      "+---+------------+-----------------+\n",
      "|  1|  2019-03-28|             null|\n",
      "|  2|  2021-04-02|             null|\n",
      "|  3|  2019-05-24|             null|\n",
      "|  4|  2018-10-22|             null|\n",
      "|  5|  2022-11-29|             null|\n",
      "|  6|  2018-05-26|             null|\n",
      "|  7|  2020-08-23|             null|\n",
      "|  8|  2019-02-16|             null|\n",
      "|  9|  2021-03-09|             null|\n",
      "| 10|  2022-04-09|             null|\n",
      "| 11|  2019-10-08|             null|\n",
      "| 12|  2022-04-28|             null|\n",
      "| 13|  2019-02-15|             null|\n",
      "| 14|  2022-08-21|             null|\n",
      "| 15|  2022-07-15|             null|\n",
      "| 16|  2019-12-27|             null|\n",
      "| 17|  2022-07-31|             null|\n",
      "| 18|  2018-07-13|             null|\n",
      "| 19|  2019-04-14|             null|\n",
      "| 20|  2022-12-07|             null|\n",
      "+---+------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_encerramento_csv_fmt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e0abba32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataframe Fatura\n",
    "df_fatura_csv = (\n",
    "    spark.read\n",
    "    .format('csv')\n",
    "    .option('header', True)\n",
    "    .option('delimiter', ';')\n",
    "    .load(v_diretorio_csv + 'fatura.csv')\n",
    ")\n",
    "\n",
    "#Definindo o tipo da coluna\n",
    "df_fatura_csv_fmt = (\n",
    "    df_fatura_csv\n",
    "    .withColumn('id', col('id').cast(IntegerType()))\n",
    "    .withColumn('data_vencimento_fatura', col('data_vencimento_fatura').cast(DateType()))\n",
    "    .withColumn('vlr_fatura', col('vlr_fatura').cast(DecimalType(10,2)))\n",
    "    .withColumn('data_pagamento_fatura', col('data_pagamento_fatura').cast(DateType()))\n",
    "    .withColumn('qtd_dias_atraso_pgto', col('qtd_dias_atraso_pgto').cast(IntegerType()))\n",
    "    .withColumn('id_cartao', col('id_cartao').cast(IntegerType()))\n",
    ")\n",
    "\n",
    "#Criando view do dataframe\n",
    "df_fatura_csv_fmt.createOrReplaceTempView('fatura')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "65f118eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------------+----------+---------------------+--------------------+---------+\n",
      "| id|data_vencimento_fatura|vlr_fatura|data_pagamento_fatura|qtd_dias_atraso_pgto|id_cartao|\n",
      "+---+----------------------+----------+---------------------+--------------------+---------+\n",
      "|  1|            2019-03-15|      0.00|           2019-03-15|                   0|        1|\n",
      "|  2|            2019-04-15|   1470.86|           2019-04-11|                   0|        1|\n",
      "|  3|            2019-05-15|   1634.88|           2019-05-11|                   0|        1|\n",
      "|  4|            2019-06-15|    437.91|           2019-06-11|                   0|        1|\n",
      "|  5|            2019-07-15|   1006.45|           2019-07-14|                   0|        1|\n",
      "|  6|            2019-08-15|    932.13|           2019-08-12|                   0|        1|\n",
      "|  7|            2019-09-15|    693.69|           2019-09-12|                   0|        1|\n",
      "|  8|            2019-10-15|    349.34|           2019-10-12|                   0|        1|\n",
      "|  9|            2019-11-15|   1609.69|           2019-11-11|                   0|        1|\n",
      "| 10|            2019-12-15|   1456.59|           2019-12-14|                   0|        1|\n",
      "| 11|            2020-01-15|   1129.89|           2020-01-14|                   0|        1|\n",
      "| 12|            2020-02-15|    186.44|           2020-02-15|                   0|        1|\n",
      "| 13|            2020-03-15|   1350.80|           2020-03-11|                   0|        1|\n",
      "| 14|            2020-04-15|    880.98|           2020-04-12|                   0|        1|\n",
      "| 15|            2020-05-15|   1493.61|           2020-05-12|                   0|        1|\n",
      "| 16|            2020-06-15|    518.68|           2020-06-10|                   0|        1|\n",
      "| 17|            2020-07-15|    958.56|           2020-07-11|                   0|        1|\n",
      "| 18|            2020-08-15|    210.13|           2020-08-15|                   0|        1|\n",
      "| 19|            2020-09-15|   1059.57|           2020-09-11|                   0|        1|\n",
      "| 20|            2020-10-15|   1224.60|           2020-10-13|                   0|        1|\n",
      "+---+----------------------+----------+---------------------+--------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_fatura_csv_fmt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b5f3b7",
   "metadata": {},
   "source": [
    "**Funcoes de carga dados do banco de dados do target e criando views das chaves**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd10d34",
   "metadata": {},
   "source": [
    "Essa fase do processo, serão carregados as chaves das tabelas do banco de dados do postgres em dataframes e views, para validacao se registro ja existe na base e validacao de integridade de relacionamento entre tabelas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2c127c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funcao para carregar as chaves\n",
    "def f_carrega_associado_tgt():\n",
    "    #Carregando dataframe com dados do banco de target\n",
    "    df_associado_tgt = (\n",
    "        spark.read\n",
    "        .format('jdbc')\n",
    "        .option('url', v_url_jdbc)\n",
    "        .option('query', 'select id from target.associado')\n",
    "        .option('user', v_user_jdbc)\n",
    "        .option('password', v_pass_jdbc)\n",
    "        .load()\n",
    "    )\n",
    "\n",
    "    #Criando view do dataframe\n",
    "    df_associado_tgt.createOrReplaceTempView('associado_tgt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1f50f3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funcao para carregar as chaves\n",
    "def f_carrega_conta_tgt():\n",
    "    #Carregando dataframe com dados do banco de target\n",
    "    df_conta_tgt = (\n",
    "        spark.read\n",
    "        .format('jdbc')\n",
    "        .option('url', v_url_jdbc)\n",
    "        .option(\"query\", 'select id from target.conta')\n",
    "        .option('user', v_user_jdbc)\n",
    "        .option('password', v_pass_jdbc)\n",
    "        .load()\n",
    "    )\n",
    "\n",
    "    #Criando view do dataframe\n",
    "    df_conta_tgt.createOrReplaceTempView('conta_tgt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e09dda7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funcao para carregar as chaves\n",
    "def f_carrega_cartao_tgt():\n",
    "    #Carregando dataframe com dados do banco de target\n",
    "    df_cartao_tgt = (\n",
    "        spark.read\n",
    "        .format('jdbc')\n",
    "        .option('url', v_url_jdbc)\n",
    "        .option(\"query\", 'select id from target.cartao')\n",
    "        .option('user', v_user_jdbc)\n",
    "        .option('password', v_pass_jdbc)\n",
    "        .load()\n",
    "    )\n",
    "\n",
    "    #Criando view do dataframe\n",
    "    df_cartao_tgt.createOrReplaceTempView('cartao_tgt')\n",
    "\n",
    "f_carrega_cartao_tgt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "69a19b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funcao para carregar as chaves\n",
    "def f_carrega_movimento_tgt():\n",
    "    #Carregando dataframe com dados do banco de target\n",
    "    df_movimento_tgt = (\n",
    "        spark.read\n",
    "        .format('jdbc')\n",
    "        .option('url', v_url_jdbc)\n",
    "        .option(\"query\", 'select id from target.movimento')\n",
    "        .option('user', v_user_jdbc)\n",
    "        .option('password', v_pass_jdbc)\n",
    "        .load()\n",
    "    )\n",
    "\n",
    "    #Criando view do dataframe\n",
    "    df_movimento_tgt.createOrReplaceTempView('movimento_tgt')\n",
    "\n",
    "f_carrega_movimento_tgt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8195c0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funcao para carregar as chaves\n",
    "def f_carrega_encerramento_tgt():\n",
    "    #Carregando dataframe com dados do banco de target\n",
    "    df_encerramento_tgt = (\n",
    "        spark.read\n",
    "        .format('jdbc')\n",
    "        .option('url', v_url_jdbc)\n",
    "        .option(\"query\", 'select id from target.encerramento_conta')\n",
    "        .option('user', v_user_jdbc)\n",
    "        .option('password', v_pass_jdbc)\n",
    "        .load()\n",
    "    )\n",
    "\n",
    "    #Criando view do dataframe\n",
    "    df_encerramento_tgt.createOrReplaceTempView('encerramento_conta_tgt')\n",
    "\n",
    "f_carrega_encerramento_tgt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a3c6f54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funcao para carregar as chaves\n",
    "def f_carrega_fatura_tgt():\n",
    "    #Carregando dataframe com dados do banco de target\n",
    "    df_fatura_tgt = (\n",
    "        spark.read\n",
    "        .format('jdbc')\n",
    "        .option('url', v_url_jdbc)\n",
    "        .option(\"query\", 'select id from target.fatura')\n",
    "        .option('user', v_user_jdbc)\n",
    "        .option('password', v_pass_jdbc)\n",
    "        .load()\n",
    "    )\n",
    "\n",
    "    #Criando view do dataframe\n",
    "    df_fatura_tgt.createOrReplaceTempView('fatura_tgt')\n",
    "\n",
    "f_carrega_fatura_tgt()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4d794d",
   "metadata": {},
   "source": [
    "**Verificacao, tratamento e carga de dados**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f926ac37",
   "metadata": {},
   "source": [
    "Essa fase do processo, serão validados os dados,corrigidos integridade de relacionamento entre os dados e carregados os dados no banco de dados do target.\n",
    "\n",
    "A estrategia de carga de dados, sera:\n",
    "\n",
    "Caso o dado exista, nao sobrescrever, caso seja um registro novo, inserir."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7867d74e",
   "metadata": {},
   "source": [
    "**Carga de dados do Associado**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "10211c33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Verificacao se a chave ja existe, caso nao exista, insere na tabela de target\n",
    "f_carrega_associado_tgt()\n",
    "df_associado_novo=spark.sql('''\n",
    "    select \n",
    "        wrk.*\n",
    "    from associado wrk\n",
    "    \n",
    "    left join associado_tgt tgt\n",
    "    on tgt.id=wrk.id\n",
    "    \n",
    "    where tgt.id is null\n",
    "''')\n",
    "\n",
    "#Inserindo dados novos\n",
    "(df_associado_novo.write\n",
    "    .format('jdbc')\n",
    "    .option('url', v_url_jdbc)\n",
    "    .option('dbtable', 'target.associado')\n",
    "    .option('user', v_user_jdbc)\n",
    "    .option('password', v_pass_jdbc)\n",
    "    .mode('append')\n",
    "    .save()\n",
    ")\n",
    "\n",
    "#Atualizando as chaves da view do target\n",
    "f_carrega_associado_tgt()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9365a57",
   "metadata": {},
   "source": [
    "**Carga de dados da Conta Corrente**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "82ab48f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Validacao e correcao de relacionamento entre a conta e o associado.\n",
    "#Caso o associado nao exista, sera informado -1 na coluna.\n",
    "\n",
    "df_conta_tratado=spark.sql('''\n",
    "    select\n",
    "        cco.id,\n",
    "        cco.tipo,\n",
    "        cco.data_criacao,\n",
    "        coalesce(ass.id, -1) as id_associado\n",
    "    from conta cco\n",
    "    \n",
    "    left join associado_tgt ass\n",
    "    on ass.id=cco.id_associado\n",
    "''')\n",
    "\n",
    "#Criando view do dataframe\n",
    "df_conta_tratado.createOrReplaceTempView('conta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ce3bc10a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 17:====================================>                 (137 + 1) / 200]\r",
      "\r",
      "[Stage 17:================================================>     (181 + 1) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Associados invalidos: 51. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "qtd_associados_invalidos=df_conta_tratado.filter(df_conta_tratado.id_associado==-1).count()\n",
    "\n",
    "print(f\"Associados invalidos: {qtd_associados_invalidos}. \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "48175813",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Verificacao se a chave ja existe, caso nao exista, insere na tabela de target\n",
    "f_carrega_conta_tgt()\n",
    "df_conta_nova=spark.sql('''\n",
    "    select \n",
    "        wrk.*\n",
    "    from conta wrk\n",
    "    \n",
    "    left join conta_tgt tgt\n",
    "    on tgt.id=wrk.id\n",
    "    \n",
    "    where tgt.id is null\n",
    "''')\n",
    "\n",
    "#Inserindo dados novos\n",
    "(df_conta_nova.write\n",
    "    .format('jdbc')\n",
    "    .option('url', v_url_jdbc)\n",
    "    .option('dbtable', 'target.conta')\n",
    "    .option('user', v_user_jdbc)\n",
    "    .option('password', v_pass_jdbc)\n",
    "    .mode('append').save()\n",
    ")\n",
    "\n",
    "#Atualizando as chaves da view do target\n",
    "f_carrega_conta_tgt()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee1ed66",
   "metadata": {},
   "source": [
    "**Carga de dados de Cartao**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7f1bae8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Validacao e correcao de relacionamento entre a cartao, conta e o associado.\n",
    "#Caso o associado/conta nao exista, sera informado -1 na coluna.\n",
    "\n",
    "df_cartao_tratado=spark.sql('''\n",
    "    select \n",
    "        car.id,\n",
    "        car.num_cartao,\n",
    "        car.nom_impresso,\n",
    "        coalesce(cco.id, -1) as id_conta,\n",
    "        coalesce(ass.id, -1) as id_associado\n",
    "        \n",
    "    from cartao car\n",
    "    \n",
    "    left join conta_tgt cco\n",
    "    on cco.id=car.id_conta\n",
    "    \n",
    "    left join associado_tgt ass\n",
    "    on ass.id=car.id_associado\n",
    "''')\n",
    "\n",
    "\n",
    "#Criando view do dataframe\n",
    "df_cartao_tratado.createOrReplaceTempView('cartao')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d54cbce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 32:====================================================> (195 + 1) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contas invalidas: 1. Associados invalidos: 51. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "qtd_contas_invalidos=df_cartao_tratado.filter(df_cartao_tratado.id_conta == -1).count()\n",
    "qtd_associados_invalidos=df_cartao_tratado.filter(df_cartao_tratado.id_associado == -1).count()\n",
    "\n",
    "print(f\"Contas invalidas: {qtd_contas_invalidos}. Associados invalidos: {qtd_associados_invalidos}. \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6dfacf00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Verificacao se a chave ja existe, caso nao exista, insere na tabela de target\n",
    "f_carrega_cartao_tgt()\n",
    "df_cartao_nova=spark.sql('''\n",
    "    select \n",
    "        wrk.*\n",
    "    from cartao wrk\n",
    "    \n",
    "    left join cartao_tgt tgt\n",
    "    on tgt.id=wrk.id\n",
    "    \n",
    "    where tgt.id is null\n",
    "''')\n",
    "\n",
    "#Inserindo dados novos\n",
    "(df_cartao_nova.write\n",
    "    .format('jdbc')\n",
    "    .option('url', v_url_jdbc)\n",
    "    .option('dbtable', 'target.cartao')\n",
    "    .option('user', v_user_jdbc)\n",
    "    .option('password', v_pass_jdbc)\n",
    "    .mode('append').save()\n",
    ")\n",
    "\n",
    "#Atualizando as chaves da view do target\n",
    "f_carrega_cartao_tgt()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53eb0b07",
   "metadata": {},
   "source": [
    "**Carga de dados de Movimento**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3b811c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Validacao e correcao de relacionamento entre a movimento e cartao.\n",
    "#Caso o cartao nao exista, sera informado -1 na coluna.\n",
    "\n",
    "df_movimento_tratado=spark.sql('''\n",
    "    select \n",
    "        mov.id,\n",
    "        mov.vlr_transacao,\n",
    "        mov.des_transacao,\n",
    "        mov.data_movimento,\n",
    "        coalesce(car.id, -1) as id_cartao\n",
    "        \n",
    "    from movimento mov\n",
    "    \n",
    "    left join cartao_tgt car\n",
    "    on car.id=mov.id_cartao\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e3927380",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 45:=====================================>                (140 + 1) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cartoes invalidos: 0. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 45:=================================================>    (182 + 1) / 200]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "qtd_cartoes_invalidos=df_movimento_tratado.filter(df_movimento_tratado.id_cartao == -1).count()\n",
    "\n",
    "print(f\"Cartoes invalidos: {qtd_cartoes_invalidos}. \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f3e03c8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Verificacao se a chave ja existe, caso nao exista, insere na tabela de target\n",
    "f_carrega_movimento_tgt()\n",
    "df_movimento_nova=spark.sql('''\n",
    "    select \n",
    "        wrk.*\n",
    "    from movimento wrk\n",
    "    \n",
    "    left join movimento_tgt tgt\n",
    "    on tgt.id=wrk.id\n",
    "    \n",
    "    where tgt.id is null\n",
    "''')\n",
    "\n",
    "#Inserindo dados novos\n",
    "(df_movimento_nova.write\n",
    "    .format('jdbc')\n",
    "    .option('url', v_url_jdbc)\n",
    "    .option('dbtable', 'target.movimento')\n",
    "    .option('user', v_user_jdbc)\n",
    "    .option('password', v_pass_jdbc)\n",
    "    .mode('append').save()\n",
    ")\n",
    "\n",
    "#Atualizando as chaves da view do target\n",
    "f_carrega_movimento_tgt()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8930b2e",
   "metadata": {},
   "source": [
    "**Carga de dados de Encerramento da Conta**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "07655881",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Verificacao se a chave ja existe, caso nao exista, insere na tabela de target\n",
    "f_carrega_encerramento_tgt()\n",
    "df_encerramento_nova=spark.sql('''\n",
    "    select \n",
    "        wrk.*\n",
    "    from encerramento_conta wrk\n",
    "    \n",
    "    left join encerramento_conta_tgt tgt\n",
    "    on tgt.id=wrk.id\n",
    "    \n",
    "    where tgt.id is null\n",
    "''')\n",
    "\n",
    "#Inserindo dados novos\n",
    "(df_encerramento_nova.write\n",
    "    .format('jdbc')\n",
    "    .option('url', v_url_jdbc)\n",
    "    .option('dbtable', 'target.encerramento_conta')\n",
    "    .option('user', v_user_jdbc)\n",
    "    .option('password', v_pass_jdbc)\n",
    "    .mode('append').save()\n",
    ")\n",
    "\n",
    "#Atualizando as chaves da view do target\n",
    "f_carrega_encerramento_tgt()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91047a37",
   "metadata": {},
   "source": [
    "**Carga de dados da Fatura dos cartoes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0f6647d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Validacao e correcao de relacionamento entre a fatura e cartao.\n",
    "#Caso o cartao nao exista, sera informado -1 na coluna.\n",
    "\n",
    "df_fatura_tratado=spark.sql('''\n",
    "    select \n",
    "        fat.id,\n",
    "        fat.data_vencimento_fatura,\n",
    "        fat.vlr_fatura,\n",
    "        fat.data_pagamento_fatura,\n",
    "        fat.qtd_dias_atraso_pgto,\n",
    "        coalesce(car.id, -1) as id_cartao\n",
    "        \n",
    "    from fatura fat\n",
    "    \n",
    "    left join cartao car\n",
    "    on car.id=fat.id_cartao\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "baf6ee27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 59:====================================================> (195 + 1) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cartoes invalidos: 0. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "qtd_cartoes_fat_invalidos=df_fatura_tratado.filter(df_fatura_tratado.id_cartao == -1).count()\n",
    "\n",
    "print(f\"Cartoes invalidos: {qtd_cartoes_fat_invalidos}. \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7b6bf30f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Verificacao se a chave ja existe, caso nao exista, insere na tabela de target\n",
    "f_carrega_fatura_tgt()\n",
    "df_fatura_nova=spark.sql('''\n",
    "    select \n",
    "        wrk.*\n",
    "    from fatura wrk\n",
    "    \n",
    "    left join fatura_tgt tgt\n",
    "    on tgt.id=wrk.id\n",
    "    \n",
    "    where tgt.id is null\n",
    "''')\n",
    "\n",
    "#Inserindo dados novos\n",
    "(df_fatura_nova.write\n",
    "    .format('jdbc')\n",
    "    .option('url', v_url_jdbc)\n",
    "    .option('dbtable', 'target.fatura')\n",
    "    .option('user', v_user_jdbc)\n",
    "    .option('password', v_pass_jdbc)\n",
    "    .mode('append').save()\n",
    ")\n",
    "\n",
    "#Atualizando as chaves da view do target\n",
    "f_carrega_fatura_tgt()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc50ad2c",
   "metadata": {},
   "source": [
    "## Geração do arquivo Flat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24d18b9",
   "metadata": {},
   "source": [
    "O objetivo do exercício, é exercitar o processamento no Spark. A extração dos dados e relacionamentos entre os dados, serão processados no **Spark**.\n",
    "\n",
    "Por essa razão, os dados não serão relacionados e tratados no **postgres**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7c9289d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Carregando dados no Dataframe\n",
    "df_associado_tgt = (\n",
    "    spark.read\n",
    "    .format('jdbc')\n",
    "    .option('url', v_url_jdbc)\n",
    "    .option('dbtable', 'target.associado')\n",
    "    .option('user', v_user_jdbc)\n",
    "    .option('password', v_pass_jdbc)\n",
    "    .load()\n",
    ")\n",
    "\n",
    "#Criando view do dataframe\n",
    "df_associado_tgt.createOrReplaceTempView('associado')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9189b9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Carregando dados no Dataframe\n",
    "df_conta_tgt = (\n",
    "    spark.read\n",
    "    .format('jdbc')\n",
    "    .option('url', v_url_jdbc)\n",
    "    .option('dbtable', 'target.conta')\n",
    "    .option('user', v_user_jdbc)\n",
    "    .option('password', v_pass_jdbc)\n",
    "    .load()\n",
    ")\n",
    "\n",
    "#Criando view do dataframe\n",
    "df_conta_tgt.createOrReplaceTempView('conta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a8e366ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Carregando dados no Dataframe\n",
    "df_cartao_tgt = (\n",
    "    spark.read\n",
    "    .format('jdbc')\n",
    "    .option('url', v_url_jdbc)\n",
    "    .option('dbtable', 'target.cartao')\n",
    "    .option('user', v_user_jdbc)\n",
    "    .option('password', v_pass_jdbc)\n",
    "    .load()\n",
    ")\n",
    "\n",
    "#Criando view do dataframe\n",
    "df_cartao_tgt.createOrReplaceTempView('cartao')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d2c93111",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Carregando dados no Dataframe\n",
    "df_movimento_tgt = (\n",
    "    spark.read\n",
    "    .format('jdbc')\n",
    "    .option('url', v_url_jdbc)\n",
    "    .option('dbtable', 'target.movimento')\n",
    "    .option('user', v_user_jdbc)\n",
    "    .option('password', v_pass_jdbc)\n",
    "    .load()\n",
    ")\n",
    "\n",
    "#Criando view do dataframe\n",
    "df_movimento_tgt.createOrReplaceTempView('movimento')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69050d4",
   "metadata": {},
   "source": [
    "Obs: 1 - *Somente será trabalhado com associados válidos.*\n",
    "     2 - *Na estrutura de dados gerados, não foi repassado a coluna de data da criação do cartão, temos somente a data de criação da conta, por essa razão, será utilizada a data de criação da conta, como data de criação do cartão.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e69ae25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_flat_file=spark.sql('''\n",
    "    select\n",
    "        ass.nome as nome_associado,\n",
    "        ass.sobrenome as sobrenome_associado,\n",
    "        ass.idade as idade_associado,\n",
    "        mov.vlr_transacao as vlr_transacao_movimento,\n",
    "        mov.des_transacao as des_transacao_movimento,\n",
    "        mov.data_movimento as data_movimento,\n",
    "        car.num_cartao as numero_cartao,\n",
    "        car.nom_impresso as nome_impresso_cartao,\n",
    "        cco.data_criacao as data_criacao_cartao,\n",
    "        cco.tipo as tipo_conta,\n",
    "        cco.data_criacao as data_criacao_conta\n",
    "        \n",
    "    from associado ass\n",
    "    \n",
    "    left join cartao car\n",
    "    on car.id_associado=ass.id\n",
    "    \n",
    "    left join movimento mov\n",
    "    on mov.id_cartao=car.id\n",
    "    \n",
    "    left join conta cco\n",
    "    on cco.id_associado=ass.id\n",
    "    \n",
    "    where ass.id>0\n",
    "    \n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "765b8188",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 69:=================================================>    (183 + 1) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------------+---------------+-----------------------+-----------------------+--------------+----------------+--------------------+-------------------+--------------+------------------+\n",
      "|nome_associado|sobrenome_associado|idade_associado|vlr_transacao_movimento|des_transacao_movimento|data_movimento|   numero_cartao|nome_impresso_cartao|data_criacao_cartao|    tipo_conta|data_criacao_conta|\n",
      "+--------------+-------------------+---------------+-----------------------+-----------------------+--------------+----------------+--------------------+-------------------+--------------+------------------+\n",
      "|     Valentina|             Fogaça|             23|                 159.36|           Supermercado|    2018-12-13|4250002301488069|    VALENTINA FOGAÇA|         2018-08-14|Conta Corrente|        2018-08-14|\n",
      "|     Valentina|             Fogaça|             23|                 241.92|            Restaurante|    2020-06-06|4250002301488069|    VALENTINA FOGAÇA|         2018-08-14|Conta Corrente|        2018-08-14|\n",
      "|     Valentina|             Fogaça|             23|                 154.46|                  Roupa|    2020-06-16|4250002301488069|    VALENTINA FOGAÇA|         2018-08-14|Conta Corrente|        2018-08-14|\n",
      "|     Valentina|             Fogaça|             23|                 188.04|            Restaurante|    2020-09-03|4250002301488069|    VALENTINA FOGAÇA|         2018-08-14|Conta Corrente|        2018-08-14|\n",
      "|     Valentina|             Fogaça|             23|                 196.40|      Posto combustivel|    2019-08-24|4250002301488069|    VALENTINA FOGAÇA|         2018-08-14|Conta Corrente|        2018-08-14|\n",
      "|     Valentina|             Fogaça|             23|                  59.40|            Restaurante|    2020-03-26|4250002301488069|    VALENTINA FOGAÇA|         2018-08-14|Conta Corrente|        2018-08-14|\n",
      "|     Valentina|             Fogaça|             23|                 225.65|            Restaurante|    2020-12-02|4250002301488069|    VALENTINA FOGAÇA|         2018-08-14|Conta Corrente|        2018-08-14|\n",
      "|     Valentina|             Fogaça|             23|                   2.88|               Pet shop|    2018-12-20|4250002301488069|    VALENTINA FOGAÇA|         2018-08-14|Conta Corrente|        2018-08-14|\n",
      "|     Valentina|             Fogaça|             23|                  41.43|      Posto combustivel|    2020-02-13|4250002301488069|    VALENTINA FOGAÇA|         2018-08-14|Conta Corrente|        2018-08-14|\n",
      "|     Valentina|             Fogaça|             23|                 271.61|                  Roupa|    2020-04-23|4250002301488069|    VALENTINA FOGAÇA|         2018-08-14|Conta Corrente|        2018-08-14|\n",
      "|     Valentina|             Fogaça|             23|                  63.81|            Restaurante|    2019-02-02|4250002301488069|    VALENTINA FOGAÇA|         2018-08-14|Conta Corrente|        2018-08-14|\n",
      "|     Valentina|             Fogaça|             23|                 242.82|      Posto combustivel|    2020-12-24|4250002301488069|    VALENTINA FOGAÇA|         2018-08-14|Conta Corrente|        2018-08-14|\n",
      "|     Valentina|             Fogaça|             23|                  66.45|           Supermercado|    2020-02-14|4250002301488069|    VALENTINA FOGAÇA|         2018-08-14|Conta Corrente|        2018-08-14|\n",
      "|     Valentina|             Fogaça|             23|                 293.68|               Farmacia|    2021-05-25|4250002301488069|    VALENTINA FOGAÇA|         2018-08-14|Conta Corrente|        2018-08-14|\n",
      "|     Valentina|             Fogaça|             23|                 134.12|            Restaurante|    2021-06-07|4250002301488069|    VALENTINA FOGAÇA|         2018-08-14|Conta Corrente|        2018-08-14|\n",
      "|     Valentina|             Fogaça|             23|                  31.12|                  Roupa|    2018-12-10|4250002301488069|    VALENTINA FOGAÇA|         2018-08-14|Conta Corrente|        2018-08-14|\n",
      "|     Valentina|             Fogaça|             23|                 378.84|                  Roupa|    2019-11-11|4250002301488069|    VALENTINA FOGAÇA|         2018-08-14|Conta Corrente|        2018-08-14|\n",
      "|     Valentina|             Fogaça|             23|                 122.03|               Farmacia|    2021-09-04|4250002301488069|    VALENTINA FOGAÇA|         2018-08-14|Conta Corrente|        2018-08-14|\n",
      "|     Valentina|             Fogaça|             23|                 178.75|               Pet shop|    2018-12-11|4250002301488069|    VALENTINA FOGAÇA|         2018-08-14|Conta Corrente|        2018-08-14|\n",
      "|     Valentina|             Fogaça|             23|                 252.25|                  Roupa|    2019-11-25|4250002301488069|    VALENTINA FOGAÇA|         2018-08-14|Conta Corrente|        2018-08-14|\n",
      "+--------------+-------------------+---------------+-----------------------+-----------------------+--------------+----------------+--------------------+-------------------+--------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_flat_file.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c90d460e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 77:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo flat gerado: /home/jovyan/work/export/movimento_flat.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Funcao para exportar um dataframe para arquivo csv\n",
    "def f_exporta_flat_file(df, arquivo_csv):\n",
    "    #Gera arquivo flat\n",
    "    df.coalesce(1).write.mode('overwrite').options(header='True', delimiter=';').csv(v_diretorio_export + '/flatfile')\n",
    "    \n",
    "    #Renomeia arquivo e move para a pasta export\n",
    "    for arquivo in os.listdir(v_diretorio_export + '/flatfile/'):\n",
    "        a_arquivo=arquivo.split('.')\n",
    "        if(len(a_arquivo)==2 and a_arquivo[1]=='csv'):\n",
    "            shutil.move(v_diretorio_export + '/flatfile/' + arquivo, v_diretorio_export + '/' + arquivo_csv)\n",
    "            print('Arquivo flat gerado: ' + v_diretorio_export + '/' + arquivo_csv)\n",
    "\n",
    "f_exporta_flat_file(df_flat_file, 'movimento_flat.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f4660c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
