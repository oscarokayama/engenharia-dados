{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56bc2e94",
   "metadata": {},
   "source": [
    "## 3. Atividade extra"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196f7589",
   "metadata": {},
   "source": [
    "O objetivo da atividade extra, é exportar arquivos flat file, para análise de comportamento de compra, inadimplência e churn  dos associados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6b029d",
   "metadata": {},
   "source": [
    "**Importação de bibliotecas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a49bf68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_unixtime, col, to_timestamp, coalesce\n",
    "from pyspark.sql.types import StringType, IntegerType, LongType, DecimalType, DateType"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58c1ee0",
   "metadata": {},
   "source": [
    "**Variaveis do projeto**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5eb4fa0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Variaveis de conexao com postgres\n",
    "v_caminho_jar_postgres='/home/jovyan/work/jars/postgresql-9.4.1207.jar'\n",
    "v_url_jdbc='jdbc:postgresql://postgres/airflow'\n",
    "v_user_jdbc='airflow'\n",
    "v_pass_jdbc='airflow'\n",
    "\n",
    "#Diretorio de export do arquivo de flatfile\n",
    "v_diretorio_export='/home/jovyan/work/export'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a62882",
   "metadata": {},
   "source": [
    "**Criando sessao e contexto**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce28e957",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.1.2-bin-hadoop3.2/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "22/12/28 00:19:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/12/28 00:19:27 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "22/12/28 00:19:27 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    }
   ],
   "source": [
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .master('local')\n",
    "         .appName('load-postgres')\n",
    "         # Add postgres jar\n",
    "         .config('spark.driver.extraClassPath', v_caminho_jar_postgres)\n",
    "         .getOrCreate())\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0e7ce47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funcao para exportar um dataframe para arquivo csv\n",
    "def f_exporta_flat_file(df, arquivo_csv):\n",
    "    #Gera arquivo flat\n",
    "    df.coalesce(1).write.mode('overwrite').options(header='True', delimiter=';').csv(v_diretorio_export + '/flatfile')\n",
    "    \n",
    "    #Renomeia arquivo e move para a pasta export\n",
    "    for arquivo in os.listdir(v_diretorio_export + '/flatfile/'):\n",
    "        a_arquivo=arquivo.split('.')\n",
    "        if(len(a_arquivo)==2 and a_arquivo[1]=='csv'):\n",
    "            shutil.move(v_diretorio_export + '/flatfile/' + arquivo, v_diretorio_export + '/' + arquivo_csv)\n",
    "            print('Arquivo flat gerado: ' + v_diretorio_export + '/' + arquivo_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "216576d1",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o40.load.\n: org.postgresql.util.PSQLException: FATAL: database \"projeto\" does not exist\n\tat org.postgresql.core.v3.ConnectionFactoryImpl.readStartupMessages(ConnectionFactoryImpl.java:691)\n\tat org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:207)\n\tat org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:65)\n\tat org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:159)\n\tat org.postgresql.Driver.makeConnection(Driver.java:416)\n\tat org.postgresql.Driver.connect(Driver.java:283)\n\tat org.apache.spark.sql.execution.datasources.jdbc.connection.BasicConnectionProvider.getConnection(BasicConnectionProvider.scala:49)\n\tat org.apache.spark.sql.execution.datasources.jdbc.connection.ConnectionProvider$.create(ConnectionProvider.scala:68)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$createConnectionFactory$1(JdbcUtils.scala:62)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:56)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation$.getSchema(JDBCRelation.scala:226)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:35)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:355)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:325)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$3(DataFrameReader.scala:307)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:307)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:225)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2532/3093520047.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Carregando dados no Dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m df_flat_file = (\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'jdbc'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'url'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv_url_jdbc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    208\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m     def json(self, path, schema=None, primitivesAsString=None, prefersDecimal=None,\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o40.load.\n: org.postgresql.util.PSQLException: FATAL: database \"projeto\" does not exist\n\tat org.postgresql.core.v3.ConnectionFactoryImpl.readStartupMessages(ConnectionFactoryImpl.java:691)\n\tat org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:207)\n\tat org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:65)\n\tat org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:159)\n\tat org.postgresql.Driver.makeConnection(Driver.java:416)\n\tat org.postgresql.Driver.connect(Driver.java:283)\n\tat org.apache.spark.sql.execution.datasources.jdbc.connection.BasicConnectionProvider.getConnection(BasicConnectionProvider.scala:49)\n\tat org.apache.spark.sql.execution.datasources.jdbc.connection.ConnectionProvider$.create(ConnectionProvider.scala:68)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$createConnectionFactory$1(JdbcUtils.scala:62)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:56)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation$.getSchema(JDBCRelation.scala:226)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:35)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:355)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:325)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$3(DataFrameReader.scala:307)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:307)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:225)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    }
   ],
   "source": [
    "#Carregando dados no Dataframe\n",
    "df_flat_file = (\n",
    "    spark.read\n",
    "    .format('jdbc')\n",
    "    .option('url', v_url_jdbc)\n",
    "    .option('query', '''\n",
    "        select \n",
    "            ass.id as id_associado,\n",
    "            ass.nome,\n",
    "            ass.sobrenome,\n",
    "            ass.idade,\n",
    "            ass.email,\n",
    "            case when compras_inad.data_encerramento_conta is null then 'Sim' else 'Não' end as sit_associado,\n",
    "            case when coalesce(compras_inad.qtd_dias_atraso_pgto, 0)=0 then 'Não' else 'Sim' end as sit_inadimplente,\n",
    "            compras_inad.data_primeira_compra,\n",
    "            compras_inad.data_ultima_compra,\n",
    "            (compras_inad.data_ultima_compra - compras_inad.data_primeira_compra)/30::integer as qt_meses_compras,\n",
    "            compras_inad.data_encerramento_conta,\n",
    "            compras_inad.dia_ult_compra,\n",
    "            compras_inad.dia_ult_compra/30 as mes_ult_compra,\n",
    "            trim(replace(compras_inad.vlr_total_compras::varchar(100), '.', ',')) as vlr_total_compras,\n",
    "            compras_inad.qtd_compras,\n",
    "            trim(replace(compras_inad.vlr_medio_compras::varchar(100), '.', ',')) as vlr_medio_compras,\n",
    "            compras_inad.qtd_dias_atraso_pgto\n",
    "\n",
    "        from target.associado ass\n",
    "\n",
    "        left join (\n",
    "            select \n",
    "                car.id_associado,\n",
    "                min(mov.data_movimento) as data_primeira_compra,\n",
    "                max(mov.data_movimento) as data_ultima_compra,\n",
    "                enc.data_encerramento as data_encerramento_conta,\n",
    "                coalesce(enc.data_encerramento, current_date) - max(mov.data_movimento) as dia_ult_compra,\n",
    "                sum(mov.vlr_transacao) as vlr_total_compras,\n",
    "                count(distinct mov.id) as qtd_compras,\n",
    "                avg(mov.vlr_transacao)::decimal(10,2) as vlr_medio_compras,\n",
    "                max(fat.qtd_dias_atraso_pgto) as qtd_dias_atraso_pgto\n",
    "\n",
    "            from target.movimento mov\n",
    "\n",
    "            inner join target.cartao car\n",
    "            on mov.id_cartao=car.id\n",
    "\n",
    "            inner join target.encerramento_conta enc\n",
    "            on enc.id=car.id_conta\n",
    "\n",
    "            inner join (\n",
    "                select\n",
    "                    id_cartao,\n",
    "                    max(qtd_dias_atraso_pgto) as qtd_dias_atraso_pgto\n",
    "                from target.fatura\n",
    "\n",
    "                group by 1\n",
    "            ) fat\n",
    "            on fat.id_cartao=car.id\n",
    "\n",
    "            group by car.id_associado, enc.data_encerramento \n",
    "        ) as compras_inad\n",
    "        on compras_inad.id_associado=ass.id\n",
    "\n",
    "        where ass.id>0\n",
    "    ''')\n",
    "    .option('user', v_user_jdbc)\n",
    "    .option('password', v_pass_jdbc)\n",
    "    .load()\n",
    ")\n",
    "\n",
    "#Gerando flat file\n",
    "f_exporta_flat_file(df_flat_file, 'associado_compras_fatura_inad.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661f9b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Carregando dados no Dataframe\n",
    "df_flat_file = (\n",
    "    spark.read\n",
    "    .format('jdbc')\n",
    "    .option('url', v_url_jdbc)\n",
    "    .option('query', '''\n",
    "        select \n",
    "            ass.id as id_associado,\n",
    "            ass.nome,\n",
    "            ass.sobrenome,\n",
    "            ass.idade,\n",
    "            ass.email,\n",
    "            compras_inad.des_transacao,\n",
    "            trim(replace(compras_inad.vlr_total_compras::varchar(100), '.', ',')) as vlr_total_compras,\n",
    "            compras_inad.qtd_compras,\n",
    "            trim(replace(compras_inad.vlr_medio_compras::varchar(100), '.', ',')) as vlr_medio_compras\n",
    "\n",
    "        from target.associado ass\n",
    "\n",
    "        left join (\n",
    "            select \n",
    "                car.id_associado,\n",
    "                mov.des_transacao,\n",
    "                sum(mov.vlr_transacao) as vlr_total_compras,\n",
    "                count(distinct mov.id) as qtd_compras,\n",
    "                avg(mov.vlr_transacao)::decimal(10,2) as vlr_medio_compras\n",
    "            from target.movimento mov\n",
    "\n",
    "            inner join target.cartao car\n",
    "            on mov.id_cartao=car.id\n",
    "\n",
    "            group by 1, 2\n",
    "        ) as compras_inad\n",
    "        on compras_inad.id_associado=ass.id\n",
    "\n",
    "        where ass.id>0\n",
    "    ''')\n",
    "    .option('user', v_user_jdbc)\n",
    "    .option('password', v_pass_jdbc)\n",
    "    .load()\n",
    ")\n",
    "\n",
    "#Gerando flat file\n",
    "f_exporta_flat_file(df_flat_file, 'associado_comportamento_compras.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc94220",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
